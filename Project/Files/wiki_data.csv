title,content
Science,"



Science is a systematic discipline that builds and organises knowledge in the form of testable hypotheses and predictions about the universe.[1][2] Modern science is typically divided into two or three major branches:[3] the natural sciences (e.g., physics, chemistry, and biology), which study the physical world; and the behavioural sciences (e.g., economics, psychology, and sociology), which study individuals and societies.[4][5] The formal sciences (e.g., logic, mathematics, and theoretical computer science), which study formal systems governed by axioms and rules,[6][7] are sometimes described as being sciences as well; however, they are often regarded as a separate field because they rely on deductive reasoning instead of the scientific method or empirical evidence as their main methodology.[8][9] Applied sciences are disciplines that use scientific knowledge for practical purposes, such as engineering and medicine.[10][11][12]

The history of science spans the majority of the historical record, with the earliest identifiable predecessors to modern science dating to the Bronze Age in Egypt and Mesopotamia (c. 3000–1200 BCE). Their contributions to mathematics, astronomy, and medicine entered and shaped the Greek natural philosophy of classical antiquity, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes, while further advancements, including the introduction of the Hindu–Arabic numeral system, were made during the Golden Age of India.[13]: 12 [14][15][16] Scientific research deteriorated in these regions after the fall of the Western Roman Empire during the Early Middle Ages (400–1000 CE), but in the Medieval renaissances (Carolingian Renaissance, Ottonian Renaissance and the Renaissance of the 12th century) scholarship flourished again. Some Greek manuscripts lost in Western Europe were preserved and expanded upon in the Middle East during the Islamic Golden Age,[17] along with the later efforts of Byzantine Greek scholars who brought Greek manuscripts from the dying Byzantine Empire to Western Europe at the start of the Renaissance.

The recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th centuries revived natural philosophy,[18][19][20] which was later transformed by the Scientific Revolution that began in the 16th century[21] as new ideas and discoveries departed from previous Greek conceptions and traditions.[22][23] The scientific method soon played a greater role in knowledge creation and it was not until the 19th century that many of the institutional and professional features of science began to take shape,[24][25] along with the changing of ""natural philosophy"" to ""natural science"".[26]

New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems.[27][28] Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions,[29] government agencies,[30] and companies.[31] The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritising the ethical and moral development of commercial products, armaments, health care, public infrastructure, and environmental protection.

The word science has been used in Middle English since the 14th century in the sense of ""the state of knowing"". The word was borrowed from the Anglo-Norman language as the suffix -cience, which was borrowed from the Latin word scientia, meaning ""knowledge, awareness, understanding"". It is a noun derivative of the Latin sciens meaning ""knowing"", and undisputedly derived from the Latin sciō, the present participle scīre, meaning ""to know"".[32]

There are many hypotheses for science's ultimate word origin. According to Michiel de Vaan, Dutch linguist and Indo-Europeanist, sciō may have its origin in the Proto-Italic language as *skije- or *skijo- meaning ""to know"", which may originate from Proto-Indo-European language as *skh1-ie, *skh1-io, meaning ""to incise"". The Lexikon der indogermanischen Verben proposed sciō is a back-formation of nescīre, meaning ""to not know, be unfamiliar with"", which may derive from Proto-Indo-European *sekH- in Latin secāre, or *skh2-, from *sḱʰeh2(i)- meaning ""to cut"".[33]

In the past, science was a synonym for ""knowledge"" or ""study"", in keeping with its Latin origin. A person who conducted scientific research was called a ""natural philosopher"" or ""man of science"".[34] In 1834, William Whewell introduced the term scientist in a review of Mary Somerville's book On the Connexion of the Physical Sciences,[35] crediting it to ""some ingenious gentleman"" (possibly himself).[36]

Science has no single origin. Rather, scientific thinking emerged gradually over the course of tens of thousands of years,[37][38] taking different forms around the world, and few details are known about the very earliest developments. Women likely played a central role in prehistoric science,[39] as did religious rituals.[40] Some scholars use the term ""protoscience"" to label activities in the past that resemble modern science in some but not all features;[41][42][43] however, this label has also been criticised as denigrating,[44] or too suggestive of presentism, thinking about those activities only in relation to modern categories.[45]

Direct evidence for scientific processes becomes clearer with the advent of writing systems in the Bronze Age civilisations of Ancient Egypt and Mesopotamia (c. 3000–1200 BCE), creating the earliest written records in the history of science.[13]: 12–15 [14] Although the words and concepts of ""science"" and ""nature"" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine.[46][13]: 12  From the 3rd millennium BCE, the ancient Egyptians developed a non-positional decimal numbering system,[47] solved practical problems using geometry,[48] and developed a calendar.[49] Their healing therapies involved drug treatments and the supernatural, such as prayers, incantations, and rituals.[13]: 9 

The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing.[50] They studied animal physiology, anatomy, behaviour, and astrology for divinatory purposes.[51] The Mesopotamians had an intense interest in medicine and the earliest medical prescriptions appeared in Sumerian during the Third Dynasty of Ur.[50][52] They seem to have studied scientific subjects which had practical or religious applications and had little interest in satisfying curiosity.[50]

In classical antiquity, there is no real ancient analogue of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time.[53] Before the invention or discovery of the concept of phusis or nature by the pre-Socratic philosophers, the same words tend to be used to describe the natural ""way"" in which a plant grows,[54] and the ""way"" in which, for example, one tribe worships a particular god. For this reason, it is claimed that these men were the first philosophers in the strict sense and the first to clearly distinguish ""nature"" and ""convention"".[55]

The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural.[56] The Pythagoreans developed a complex number philosophy[57]: 467–468  and contributed significantly to the development of mathematical science.[57]: 465  The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus.[58][59] Later, Epicurus would develop a full natural cosmology based on atomism, and would adopt a ""canon"" (ruler, standard) which established physical criteria or standards of scientific truth.[60] The Greek doctor Hippocrates established the tradition of systematic medical science[61][62] and is known as ""The Father of Medicine"".[63]

A turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. The Socratic method searches for general commonly-held truths that shape beliefs and scrutinises them for consistency.[64] Socrates criticised the older type of study of physics as too purely speculative and lacking in self-criticism.[65]

In the 4th century BCE, Aristotle created a systematic programme of teleological philosophy.[66] In the 3rd century BCE, Greek astronomer Aristarchus of Samos was the first to propose a heliocentric model of the universe, with the Sun at the centre and all the planets orbiting it.[67] Aristarchus's model was widely rejected because it was believed to violate the laws of physics,[67] while Ptolemy's Almagest, which contains a geocentric description of the Solar System, was accepted through the early Renaissance instead.[68][69] The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus.[70] Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopaedia Natural History.[71][72][73]

Positional notation for representing numbers likely emerged between the 3rd and 5th centuries CE along Indian trade routes. This numeral system made efficient arithmetic operations more accessible and would eventually become standard for mathematics worldwide.[74]

Due to the collapse of the Western Roman Empire, the 5th century saw an intellectual decline, with knowledge of classical Greek conceptions of the world deteriorating in Western Europe.[13]: 194  Latin encyclopaedists of the period such as Isidore of Seville preserved the majority of general ancient knowledge.[75] In contrast, because the Byzantine Empire resisted attacks from invaders, they were able to preserve and improve prior learning.[13]: 159  John Philoponus, a Byzantine scholar in the 6th century, started to question Aristotle's teaching of physics, introducing the theory of impetus.[13]: 307, 311, 363, 402  His criticism served as an inspiration to medieval scholars and Galileo Galilei, who extensively cited his works ten centuries later.[13]: 307–308 [76]

During late antiquity and the Early Middle Ages, natural phenomena were mainly examined via the Aristotelian approach. The approach includes Aristotle's four causes: material, formal, moving, and final cause.[77] Many Greek classical texts were preserved by the Byzantine Empire and Arabic translations were done by groups such as the Nestorians and the Monophysites. Under the Abbasids, these Arabic translations were later improved and developed by Arabic scientists.[78] By the 6th and 7th centuries, the neighbouring Sasanian Empire established the medical Academy of Gondishapur, which was considered by Greek, Syriac, and Persian physicians as the most important medical hub of the ancient world.[79]

Islamic study of Aristotelianism flourished in the House of Wisdom established in the Abbasid capital of Baghdad, Iraq[80] and the flourished[81] until the Mongol invasions in the 13th century. Ibn al-Haytham, better known as Alhazen, used controlled experiments in his optical study.[a][83][84] Avicenna's compilation of The Canon of Medicine, a medical encyclopaedia, is considered to be one of the most important publications in medicine and was used until the 18th century.[85]

By the 11th century, most of Europe had become Christian,[13]: 204  and in 1088, the University of Bologna emerged as the first university in Europe.[86] As such, demand for Latin translation of ancient and scientific texts grew,[13]: 204  a major contributor to the Renaissance of the 12th century. Renaissance scholasticism in western Europe flourished, with experiments done by observing, describing, and classifying subjects in nature.[87] In the 13th century, medical teachers and students at Bologna began opening human bodies, leading to the first anatomy textbook based on human dissection by Mondino de Luzzi.[88]

New developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. At the start of the Renaissance, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle.[82]: Book I  A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final.[89]

In the 16th century, Nicolaus Copernicus formulated a heliocentric model of the Solar System, stating that the planets revolve around the Sun, instead of the geocentric model where the planets and the Sun revolve around the Earth. This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model.[90]

Johannes Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light.[89][91] Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics and described his work as a search for the Harmony of the Spheres.[92] Galileo had made significant contributions to astronomy, physics and engineering. However, he became persecuted after Pope Urban VIII sentenced him for writing about the heliocentric model.[93]

The printing press was widely used to publish scholarly arguments, including some that disagreed widely with contemporary ideas of nature.[94] Francis Bacon and René Descartes published philosophical arguments in favour of a new type of non-Aristotelian science. Bacon emphasised the importance of experiment over contemplation, questioned the Aristotelian concepts of formal and final cause, promoted the idea that science should study the laws of nature and the improvement of all human life.[95] Descartes emphasised individual thought and argued that mathematics rather than geometry should be used to study nature.[96]

At the start of the Age of Enlightenment, Isaac Newton formed the foundation of classical mechanics by his Philosophiæ Naturalis Principia Mathematica, greatly influencing future physicists.[97] Gottfried Wilhelm Leibniz incorporated terms from Aristotelian physics, now used in a new non-teleological way. This implied a shift in the view of objects: objects were now considered as having no innate goals. Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes.[98]

During this time the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words, ""the real and legitimate goal of sciences is the endowment of human life with new inventions and riches"", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond ""the fume of subtle, sublime or pleasing [speculation]"".[99]

Science during the Enlightenment was dominated by scientific societies and academies,[100] which had largely replaced universities as centres of scientific research and development. Societies and academies were the backbones of the maturation of the scientific profession. Another important development was the popularisation of science among an increasingly literate population.[101] Enlightenment philosophers turned to a few of their scientific predecessors – Galileo, Kepler, Boyle, and Newton principally – as the guides to every physical and social field of the day.[102][103]

The 18th century saw significant advancements in the practice of medicine[104] and physics;[105] the development of biological taxonomy by Carl Linnaeus;[106] a new understanding of magnetism and electricity;[107] and the maturation of chemistry as a discipline.[108] Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed A Treatise of Human Nature, which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity.[109] Modern sociology largely originated from this movement.[110] In 1776, Adam Smith published The Wealth of Nations, which is often considered the first work on modern economics.[111]

During the 19th century, many distinguishing characteristics of contemporary modern science began to take shape. These included the transformation of the life and physical sciences; the frequent use of precision instruments; the emergence of terms such as ""biologist"", ""physicist"", and ""scientist""; an increased professionalisation of those studying nature; scientists gaining cultural authority over many dimensions of society; the industrialisation of numerous countries; the thriving of popular science writings; and the emergence of science journals.[112] During the late 19th century, psychology emerged as a separate discipline from philosophy when Wilhelm Wundt founded the first laboratory for psychological research in 1879.[113]

During the mid-19th century Charles Darwin and Alfred Russel Wallace independently proposed the theory of evolution by natural selection in 1858, which explained how different plants and animals originated and evolved. Their theory was set out in detail in Darwin's book On the Origin of Species, published in 1859.[114] Separately, Gregor Mendel presented his paper, ""Experiments on Plant Hybridisation"" in 1865,[115] which outlined the principles of biological inheritance, serving as the basis for modern genetics.[116]

Early in the 19th century John Dalton suggested the modern atomic theory, based on Democritus's original idea of indivisible particles called atoms.[117] The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. However, with the advent of the steam engine and the Industrial Revolution there was an increased understanding that not all forms of energy have the same energy qualities, the ease of conversion to useful work or to another form of energy.[118] This realisation led to the development of the laws of thermodynamics, in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time.[b]

The electromagnetic theory was established in the 19th century by the works of Hans Christian Ørsted, André-Marie Ampère, Michael Faraday, James Clerk Maxwell, Oliver Heaviside, and Heinrich Hertz. The new theory raised questions that could not easily be answered using Newton's framework. The discovery of X-rays inspired the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896,[121] Marie Curie then became the first person to win two Nobel Prizes.[122] In the next year came the discovery of the first subatomic particle, the electron.[123]

In the first half of the century the development of antibiotics and artificial fertilisers improved human living standards globally.[124][125] Harmful environmental issues such as ozone depletion, ocean acidification, eutrophication, and climate change came to the public's attention and caused the onset of environmental studies.[126]

During this period scientific experimentation became increasingly larger in scale and funding.[127] The extensive technological innovation stimulated by World War I, World War II, and the Cold War led to competitions between global powers, such as the Space Race and nuclear arms race.[128][129] Substantial international collaborations were also made, despite armed conflicts.[130]

In the late 20th century active recruitment of women and elimination of sex discrimination greatly increased the number of women scientists, but large gender disparities remained in some fields.[131] The discovery of the cosmic microwave background in 1964[132] led to a rejection of the steady-state model of the universe in favour of the Big Bang theory of Georges Lemaître.[133]

The century saw fundamental changes within science disciplines. Evolution became a unified theory in the early 20th-century when the modern synthesis reconciled Darwinian evolution with classical genetics.[134] Albert Einstein's theory of relativity and the development of quantum mechanics complement classical mechanics to describe physics in extreme length, time and gravity.[135][136] Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematisation of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling.[137]

The Human Genome Project was completed in 2003 by identifying and mapping all of the genes of the human genome.[138] The first induced pluripotent human stem cells were made in 2006, allowing adult cells to be transformed into stem cells and turn into any cell type found in the body.[139] With the affirmation of the Higgs boson discovery in 2013, the last particle predicted by the Standard Model of particle physics was found.[140] In 2015, gravitational waves, predicted by general relativity a century before, were first observed.[141][142] In 2019, the international collaboration Event Horizon Telescope presented the first direct image of a black hole's accretion disc.[143]

Modern science is commonly divided into three major branches: natural science, social science, and formal science.[3] Each of these branches comprises various specialised yet overlapping scientific disciplines that often possess their own nomenclature and expertise.[144] Both natural and social sciences are empirical sciences,[145] as their knowledge is based on empirical observations and is capable of being tested for its validity by other researchers working under the same conditions.[146]

Natural science is the study of the physical world. It can be divided into two main branches: life science and physical science. These two branches may be further divided into more specialised disciplines. For example, physical science can be subdivided into physics, chemistry, astronomy, and earth science. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches that were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science.[147] Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and other biotic beings.[148] Today, ""natural history"" suggests observational descriptions aimed at popular audiences.[149]

Social science is the study of human behaviour and the functioning of societies.[4][5] It has many disciplines that include, but are not limited to anthropology, economics, history, human geography, political science, psychology, and sociology.[4] In the social sciences, there are many competing theoretical perspectives, many of which are extended through competing research programmes such as the functionalists, conflict theorists, and interactionists in sociology.[4] Due to the limitations of conducting controlled experiments involving large groups of individuals or complex situations, social scientists may adopt other research methods such as the historical method, case studies, and cross-cultural studies. Moreover, if quantitative information is available, social scientists may rely on statistical approaches to better understand social relationships and processes.[4]

Formal science is an area of study that generates knowledge using formal systems.[150][6][7] A formal system is an abstract structure used for inferring theorems from axioms according to a set of rules.[151] It includes mathematics,[152][153] systems theory, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts.[8][154][146] The formal sciences are therefore a priori disciplines and because of this, there is disagreement on whether they constitute a science.[155][156] Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics.[157] Natural and social sciences that rely heavily on mathematical applications include mathematical physics,[158] chemistry,[159] biology,[160] finance,[161] and economics.[162]

Applied science is the use of the scientific method and knowledge to attain practical goals and includes a broad range of disciplines such as engineering and medicine.[163][12] Engineering is the use of scientific principles to invent, design and build machines, structures and technologies.[164] Science may contribute to the development of new technologies.[165] Medicine is the practice of caring for patients by maintaining and restoring health through the prevention, diagnosis, and treatment of injury or disease.[166][167] The applied sciences are often contrasted with the basic sciences, which are focused on advancing scientific theories and laws that explain and predict events in the natural world.[168][169]

Computational science applies computing power to simulate real-world situations, enabling a better understanding of scientific problems than formal mathematics alone can achieve. The use of machine learning and artificial intelligence is becoming a central feature of computational contributions to science, for example in agent-based computational economics, random forests, topic modeling and various forms of prediction. However, machines alone rarely advance knowledge as they require human guidance and capacity to reason; and they can introduce bias against certain social groups or sometimes underperform against humans.[170][171]

Interdisciplinary science involves the combination of two or more disciplines into one,[172] such as bioinformatics, a combination of biology and computer science[173] or cognitive sciences. The concept has existed since the ancient Greek period and it became popular again in the 20th century.[174]

Scientific research can be labelled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Most understanding comes from basic research, though sometimes applied research targets specific practical problems. This leads to technological advances that were not previously imaginable.[175]

Scientific research involves using the scientific method, which seeks to objectively explain the events of nature in a reproducible way.[176] Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: there is an objective reality shared by all rational observers; this objective reality is governed by natural laws; these laws were discovered by means of systematic observation and experimentation.[2] Mathematics is essential in the formation of hypotheses, theories, and laws, because it is used extensively in quantitative modelling, observing, and collecting measurements.[177] Statistics is used to summarise and analyse data, which allows scientists to assess the reliability of experimental results.[178]

In the scientific method an explanatory thought experiment or hypothesis is put forward as an explanation using parsimony principles and is expected to seek consilience – fitting with other accepted facts related to an observation or scientific question.[179] This tentative explanation is used to make falsifiable predictions, which are typically posted before being tested by experimentation. Disproof of a prediction is evidence of progress.[176]: 4–5 [180] Experimentation is especially important in science to help establish causal relationships to avoid the correlation fallacy, though in some sciences such as astronomy or geology, a predicted observation might be more appropriate.[181]

When a hypothesis proves unsatisfactory it is modified or discarded. If the hypothesis survives testing, it may become adopted into the framework of a scientific theory, a validly reasoned, self-consistent model or framework for describing the behaviour of certain natural events. A theory typically describes the behaviour of much broader sets of observations than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus, a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. Scientists may generate a model, an attempt to describe or depict an observation in terms of a logical, physical or mathematical representation, and to generate new hypotheses that can be tested by experimentation.[182]

While performing experiments to test hypotheses, scientists may have a preference for one outcome over another.[183][184] Eliminating the bias can be achieved through transparency, careful experimental design, and a thorough peer review process of the experimental results and conclusions.[185][186] After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be.[187] Taken in its entirety, the scientific method allows for highly creative problem solving while minimising the effects of subjective and confirmation bias.[188] Intersubjective verifiability, the ability to reach a consensus and reproduce results, is fundamental to the creation of all scientific knowledge.[189]

Scientific research is published in a range of literature.[190] Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals, Journal des sçavans followed by Philosophical Transactions, began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500.[191]

Most scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a scientific paper. Science has become so pervasive in modern societies that it is considered necessary to communicate the achievements, news, and ambitions of scientists to a wider population.[192]

The replication crisis is an ongoing methodological crisis that affects parts of the social and life sciences. In subsequent investigations, the results of many scientific studies have been proven to be unrepeatable.[193] The crisis has long-standing roots; the phrase was coined in the early 2010s[194] as part of a growing awareness of the problem. The replication crisis represents an important body of research in metascience, which aims to improve the quality of all scientific research while reducing waste.[195]

An area of study or speculation that masquerades as science in an attempt to claim legitimacy that it would not otherwise be able to achieve is sometimes referred to as pseudoscience, fringe science, or junk science.[196][197] Physicist Richard Feynman coined the term ""cargo cult science"" for cases in which researchers believe, and at a glance, look like they are doing science but lack the honesty to allow their results to be rigorously evaluated.[198] Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has been described as ""the most important tool"" for separating valid claims from invalid ones.[199]

There can also be an element of political or ideological bias on all sides of scientific debates. Sometimes, research may be characterised as ""bad science"", research that may be well-intended but is incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term ""scientific misconduct"" refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person.[200]


There are different schools of thought in the philosophy of science. The most popular position is empiricism, which holds that knowledge is created by a process involving observation; scientific theories generalise observations.[201] Empiricism generally encompasses inductivism, a position that explains how general theories can be made from the finite amount of empirical evidence available. Many versions of empiricism exist, with the predominant ones being Bayesianism and the hypothetico-deductive method.[202][201]

Empiricism has stood in contrast to rationalism, the position originally associated with Descartes, which holds that knowledge is created by the human intellect, not by observation.[203] Critical rationalism is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher Karl Popper. Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories, and that the only way theory A can be affected by observation is after theory A were to conflict with observation, but theory B were to survive the observation.[204]
Popper proposed replacing verifiability with falsifiability as the landmark of scientific theories, replacing induction with falsification as the empirical method.[204] Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism, trial and error,[205] covering all products of the human mind, including science, mathematics, philosophy, and art.[206]

Another approach, instrumentalism, emphasises the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes, with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be things that should be ignored.[207] Close to instrumentalism is constructive empiricism, according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true.[208]

Thomas Kuhn argued that the process of observation and evaluation takes place within a paradigm, a logically consistent ""portrait"" of the world that is consistent with observations made from its framing. He characterised normal science as the process of observation and ""puzzle solving"", which takes place within a paradigm, whereas revolutionary science occurs when one paradigm overtakes another in a paradigm shift.[209] Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more ""portraits"" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn's position, however, is not one of relativism.[210]

Another approach often cited in debates of scientific scepticism against controversial movements like ""creation science"" is methodological naturalism. Naturalists maintain that a difference should be made between natural and supernatural, and science should be restricted to natural explanations.[211] Methodological naturalism maintains that science requires strict adherence to empirical study and independent verification.[212]

The scientific community is a network of interacting scientists who conduct scientific research. The community consists of smaller groups working in scientific fields. By having peer review, through discussion and debate within journals and conferences, scientists maintain the quality of research methodology and objectivity when interpreting results.[213]

Scientists are individuals who conduct scientific research to advance knowledge in an area of interest.[214][215] In modern times, many professional scientists are trained in an academic setting and, upon completion, attain an academic degree, with the highest degree being a doctorate (e.g. a Doctor of Philosophy, or PhD).[216] Many scientists pursue careers in various sectors of the economy such as academia, industry, government, and nonprofit organisations.[217][218][219]

Scientists exhibit a strong curiosity about reality and a desire to apply scientific knowledge for the benefit of health, nations, the environment, or industries. Other motivations include recognition by their peers and prestige. In modern times, many scientists have advanced degrees in an area of science and pursue careers in various sectors of the economy, such as academia, industry, government, and nonprofit environments.[220][221][222]

Science has historically been a male-dominated field, with notable exceptions. Women in science faced considerable discrimination in science, much as they did in other areas of male-dominated societies. For example, women were frequently passed over for job opportunities and denied credit for their work.[223] The achievements of women in science have been attributed to the defiance of their traditional role as labourers within the domestic sphere.[224]

Learned societies for the communication and promotion of scientific thought and experimentation have existed since the Renaissance.[225] Many scientists belong to a learned society that promotes their respective scientific discipline, profession, or group of related disciplines.[226] Membership may either be open to all, require possession of scientific credentials, or conferred by election.[227] Most scientific societies are nonprofit organisations,[228] and many are professional associations. Their activities typically include holding regular conferences for the presentation and discussion of new research results and publishing or sponsoring academic journals in their discipline. Some societies act as professional bodies, regulating the activities of their members in the public interest, or the collective interest of the membership.

The professionalisation of science, begun in the 19th century, was partly enabled by the creation of national distinguished academies of sciences such as the Italian Accademia dei Lincei in 1603,[229] the British Royal Society in 1660,[230] the French Academy of Sciences in 1666,[231] the American National Academy of Sciences in 1863,[232] the German Kaiser Wilhelm Society in 1911,[233] and the Chinese Academy of Sciences in 1949.[234] International scientific organisations, such as the International Science Council, are devoted to international cooperation for science advancement.[235]

Science awards are usually given to individuals or organisations that have made significant contributions to a discipline. They are often given by prestigious institutions; thus, it is considered a great honour for a scientist receiving them. Since the early Renaissance, scientists have often been awarded medals, money, and titles. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, and chemistry.[236]

Scientific research is often funded through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most developed countries is between 1.5% and 3% of GDP.[237] In the OECD, around two-thirds of research and development in scientific and technical fields is carried out by industry, and 20% and 10%, respectively, by universities and government. The government funding proportion in certain fields is higher, and it dominates research in social science and the humanities. In less developed nations, the government provides the bulk of the funds for their basic scientific research.[238]

Many governments have dedicated agencies to support scientific research, such as the National Science Foundation in the United States,[239] the National Scientific and Technical Research Council in Argentina,[240] Commonwealth Scientific and Industrial Research Organisation in Australia,[241] National Centre for Scientific Research in France,[242] the Max Planck Society in Germany,[243] and National Research Council in Spain.[244] In commercial research and development, all but the most research-orientated corporations focus more heavily on near-term commercialisation possibilities than research driven by curiosity.[245]

Science policy is concerned with policies that affect the conduct of the scientific enterprise, including research funding, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care, and environmental monitoring. Science policy sometimes refers to the act of applying scientific knowledge and consensus to the development of public policies. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public.[246] Public policy can directly affect the funding of capital equipment and intellectual infrastructure for industrial research by providing tax incentives to those organisations that fund research.[192]

Science education for the general public is embedded in the school curriculum, and is supplemented by online pedagogical content (for example, YouTube and Khan Academy), museums, and science magazines and blogs. Major organisations of scientists such as the American Association for the Advancement of Science (AAAS) consider the sciences to be a part of the liberal arts traditions of learning, along with philosophy and history.[247] Scientific literacy is chiefly concerned with an understanding of the scientific method, units and methods of measurement, empiricism, a basic understanding of statistics (correlations, qualitative versus quantitative observations, aggregate statistics), and a basic understanding of core scientific fields such as physics, chemistry, biology, ecology, geology, and computation. As a student advances into higher stages of formal education, the curriculum becomes more in depth. Traditional subjects usually included in the curriculum are natural and formal sciences, although recent movements include social and applied science as well.[248]

The mass media face pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a scientific debate may require considerable expertise regarding the matter.[249] Few journalists have real scientific knowledge, and even beat reporters who are knowledgeable about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover.[250][251]

Science magazines such as New Scientist, Science & Vie, and Scientific American cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research.[252] The science fiction genre, primarily speculative fiction, can transmit the ideas and methods of science to the general public.[253] Recent efforts to intensify or develop links between science and non-scientific disciplines, such as literature or poetry, include the Creative Writing Science resource developed through the Royal Literary Fund.[254]

While the scientific method is broadly accepted in the scientific community, some fractions of society reject certain scientific positions or are sceptical about science. Examples are the common notion that COVID-19 is not a major health threat to the US (held by 39% of Americans in August 2021)[255] or the belief that climate change is not a major threat to the US (also held by 40% of Americans, in late 2019 and early 2020).[256] Psychologists have pointed to four factors driving rejection of scientific results:[257]

Anti-science attitudes often seem to be caused by fear of rejection in social groups. For instance, climate change is perceived as a threat by only 22% of Americans on the right side of the political spectrum, but by 85% on the left.[259] That is, if someone on the left would not consider climate change as a threat, this person may face contempt and be rejected in that social group. In fact, people may rather deny a scientifically accepted fact than lose or jeopardise their social status.[260]

Attitudes towards science are often determined by political opinions and goals. Government, business and advocacy groups have been known to use legal and economic pressure to influence scientific researchers. Many factors can act as facets of the politicisation of science such as anti-intellectualism, perceived threats to religious beliefs, and fear for business interests.[262] Politicisation of science is usually accomplished when scientific information is presented in a way that emphasises the uncertainty associated with the scientific evidence.[263] Tactics such as shifting conversation, failing to acknowledge facts, and capitalising on doubt of scientific consensus have been used to gain more attention for views that have been undermined by scientific evidence.[264] Examples of issues that have involved the politicisation of science include the global warming controversy, health effects of pesticides, and health effects of tobacco.[264][265]
"
Technology,"

Technology is the application of conceptual knowledge to achieve practical goals, especially in a reproducible way.[1] The word technology can also mean the products resulting from such efforts,[2][3] including both tangible tools such as utensils or machines, and intangible ones such as software. Technology plays a critical role in science, engineering, and everyday life.

Technological advancements have led to significant changes in society. The earliest known technology is the stone tool, used during prehistory, followed by the control of fire—which in turn contributed to the growth of the human brain and the development of language during the Ice Age, according to the cooking hypothesis. The invention of the wheel in the Bronze Age allowed greater travel and the creation of more complex machines. More recent technological inventions, including the printing press, telephone, and the Internet, have lowered barriers to communication and ushered in the knowledge economy.

While technology contributes to economic development and improves human prosperity, it can also have negative impacts like pollution and resource depletion, and can cause social harms like technological unemployment resulting from automation. As a result, philosophical and political debates about the role and use of technology, the ethics of technology, and ways to mitigate its downsides are ongoing.

Technology is a term dating back to the early 17th century that meant 'systematic treatment' (from Greek Τεχνολογία, from the Greek: τέχνη, romanized: tékhnē, lit. 'craft, art' and -λογία (-logíā), 'study, knowledge').[4][5] It is predated in use by the Ancient Greek word τέχνη (tékhnē), used to mean 'knowledge of how to make things', which encompassed activities like architecture.[6]

Starting in the 19th century, continental Europeans started using the terms Technik (German) or technique (French) to refer to a 'way of doing', which included all technical arts, such as dancing, navigation, or printing, whether or not they required tools or instruments.[7] At the time, Technologie (German and French) referred either to the academic discipline studying the ""methods of arts and crafts"", or to the political discipline ""intended to legislate on the functions of the arts and crafts.""[8] The distinction between Technik and Technologie is absent in English, and so both were translated as technology. The term was previously uncommon in English and mostly referred to the academic discipline, as in the Massachusetts Institute of Technology.[9]

In the 20th century, as a result of scientific progress and the Second Industrial Revolution, technology stopped being considered a distinct academic discipline and took on the meaning: the systemic use of knowledge to practical ends.[10]

Tools were initially developed by hominids through observation and trial and error.[11] Around 2 Mya (million years ago), they learned to make the first stone tools by hammering flakes off a pebble, forming a sharp hand axe.[12] This practice was refined 75 kya (thousand years ago) into pressure flaking, enabling much finer work.[13]

The discovery of fire was described by Charles Darwin as ""possibly the greatest ever made by man"".[14] Archaeological, dietary, and social evidence point to ""continuous [human] fire-use"" at least 1.5 Mya.[15] Fire, fueled with wood and charcoal, allowed early humans to cook their food to increase its digestibility, improving its nutrient value and broadening the number of foods that could be eaten.[16] The cooking hypothesis proposes that the ability to cook promoted an increase in hominid brain size, though some researchers find the evidence inconclusive.[17] Archaeological evidence of hearths was dated to 790 kya; researchers believe this is likely to have intensified human socialization and may have contributed to the emergence of language.[18][19]

Other technological advances made during the Paleolithic era include clothing and shelter.[20] No consensus exists on the approximate time of adoption of either technology, but archaeologists have found archaeological evidence of clothing 90-120 kya[21] and shelter 450 kya.[20] As the Paleolithic era progressed, dwellings became more sophisticated and more elaborate; as early as 380 kya, humans were constructing temporary wood huts.[22][23] Clothing, adapted from the fur and hides of hunted animals, helped humanity expand into colder regions; humans began to migrate out of Africa around 200 kya, initially moving to Eurasia.[24][25][26]

The Neolithic Revolution (or First Agricultural Revolution) brought about an acceleration of technological innovation, and a consequent increase in social complexity.[27] The invention of the polished stone axe was a major advance that allowed large-scale forest clearance and farming.[28] This use of polished stone axes increased greatly in the Neolithic but was originally used in the preceding Mesolithic in some areas such as Ireland.[29] Agriculture fed larger populations, and the transition to sedentism allowed for the simultaneous raising of more children, as infants no longer needed to be carried around by nomads. Additionally, children could contribute labor to the raising of crops more readily than they could participate in hunter-gatherer activities.[30][31]

With this increase in population and availability of labor came an increase in labor specialization.[32] What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations, such as Sumer, is not specifically known; however, the emergence of increasingly hierarchical social structures and specialized labor, of trade and war among adjacent cultures, and the need for collective action to overcome environmental challenges such as irrigation, are all thought to have played a role.[33]

The invention of writing led to the spread of cultural knowledge and became the basis for history, libraries, schools, and scientific research.[34]

Continuing improvements led to the furnace and bellows and provided, for the first time, the ability to smelt and forge gold, copper, silver, and lead – native metals found in relatively pure form in nature.[35] The advantages of copper tools over stone, bone and wooden tools were quickly apparent to early humans, and native copper was probably used from near the beginning of Neolithic times (about 10 kya).[36] Native copper does not naturally occur in large amounts, but copper ores are quite common and some of them produce metal easily when burned in wood or charcoal fires. Eventually, the working of metals led to the discovery of alloys such as bronze and brass (about 4,000 BCE). The first use of iron alloys such as steel dates to around 1,800 BCE.[37][38]

After harnessing fire, humans discovered other forms of energy. The earliest known use of wind power is the sailing ship; the earliest record of a ship under sail is that of a Nile boat dating to around 7,000 BCE.[39] From prehistoric times, Egyptians likely used the power of the annual flooding of the Nile to irrigate their lands, gradually learning to regulate much of it through purposely built irrigation channels and ""catch"" basins.[40] The ancient Sumerians in Mesopotamia used a complex system of canals and levees to divert water from the Tigris and Euphrates rivers for irrigation.[41]

Archaeologists estimate that the wheel was invented independently and concurrently in Mesopotamia (in present-day Iraq), the Northern Caucasus (Maykop culture), and Central Europe.[42] Time estimates range from 5,500 to 3,000 BCE with most experts putting it closer to 4,000 BCE.[43] The oldest artifacts with drawings depicting wheeled carts date from about 3,500 BCE.[44] More recently, the oldest-known wooden wheel in the world as of 2024 was found in the Ljubljana Marsh of Slovenia; Austrian experts have established that the wheel is between 5,100 and 5,350 years old.[45]

The invention of the wheel revolutionized trade and war. It did not take long to discover that wheeled wagons could be used to carry heavy loads. The ancient Sumerians used a potter's wheel and may have invented it.[46] A stone pottery wheel found in the city-state of Ur dates to around 3,429 BCE,[47] and even older fragments of wheel-thrown pottery have been found in the same area.[47] Fast (rotary) potters' wheels enabled early mass production of pottery, but it was the use of the wheel as a transformer of energy (through water wheels, windmills, and even treadmills) that revolutionized the application of nonhuman power sources. The first two-wheeled carts were derived from travois[48] and were first used in Mesopotamia and Iran in around 3,000 BCE.[48]


The oldest known constructed roadways are the stone-paved streets of the city-state of Ur, dating to c. 4,000 BCE,[49] and timber roads leading through the swamps of Glastonbury, England, dating to around the same period.[49] The first long-distance road, which came into use around 3,500 BCE,[49] spanned 2,400 km from the Persian Gulf to the Mediterranean Sea,[49] but was not paved and was only partially maintained.[49] In around 2,000 BCE, the Minoans on the Greek island of Crete built a 50 km road leading from the palace of Gortyn on the south side of the island, through the mountains, to the palace of Knossos on the north side of the island.[49] Unlike the earlier road, the Minoan road was completely paved.[49]
Ancient Minoan private homes had running water.[51] A bathtub virtually identical to modern ones was unearthed at the Palace of Knossos.[51][52] Several Minoan private homes also had toilets, which could be flushed by pouring water down the drain.[51] The ancient Romans had many public flush toilets,[52] which emptied into an extensive sewage system.[52] The primary sewer in Rome was the Cloaca Maxima;[52] construction began on it in the sixth century BCE and it is still in use today.[52]

The ancient Romans also had a complex system of aqueducts,[50] which were used to transport water across long distances.[50] The first Roman aqueduct was built in 312 BCE.[50] The eleventh and final ancient Roman aqueduct was built in 226 CE.[50] Put together, the Roman aqueducts extended over 450 km,[50] but less than 70 km of this was above ground and supported by arches.[50]

Innovations continued through the Middle Ages with the introduction of silk production (in Asia and later Europe), the horse collar, and horseshoes. Simple machines (such as the lever, the screw, and the pulley) were combined into more complicated tools, such as the wheelbarrow, windmills, and clocks.[53] A system of universities developed and spread scientific ideas and practices, including Oxford and Cambridge.[54]

The Renaissance era produced many innovations, including the introduction of the movable type printing press to Europe, which facilitated the communication of knowledge. Technology became increasingly influenced by science, beginning a cycle of mutual advancement.[55]

Starting in the United Kingdom in the 18th century, the discovery of steam power set off the Industrial Revolution, which saw wide-ranging technological discoveries, particularly in the areas of agriculture, manufacturing, mining, metallurgy, and transport, and the widespread application of the factory system.[56] This was followed a century later by the Second Industrial Revolution which led to rapid scientific discovery, standardization, and mass production. New technologies were developed, including sewage systems, electricity, light bulbs, electric motors, railroads, automobiles, and airplanes. These technological advances led to significant developments in medicine, chemistry, physics, and engineering.[57] They were accompanied by consequential social change, with the introduction of skyscrapers accompanied by rapid urbanization.[58] Communication improved with the invention of the telegraph, the telephone, the radio, and television.[59]

The 20th century brought a host of innovations. In physics, the discovery of nuclear fission in the Atomic Age led to both nuclear weapons and nuclear power. Analog computers were invented and asserted dominance in processing complex data. While the invention of vacuum tubes allowed for digital computing with computers like the ENIAC, their sheer size precluded widespread use until innovations in quantum physics allowed for the invention of the transistor in 1947, which significantly compacted computers and led the digital transition. Information technology, particularly optical fiber and optical amplifiers, allowed for simple and fast long-distance communication, which ushered in the Information Age and the birth of the Internet. The Space Age began with the launch of Sputnik 1 in 1957, and later the launch of crewed missions to the moon in the 1960s. Organized efforts to search for extraterrestrial intelligence have used radio telescopes to detect signs of technology use, or technosignatures, given off by alien civilizations. In medicine, new technologies were developed for diagnosis (CT, PET, and MRI scanning), treatment (like the dialysis machine, defibrillator, pacemaker, and a wide array of new pharmaceutical drugs), and research (like interferon cloning and DNA microarrays).[60]

Complex manufacturing and construction techniques and organizations are needed to make and maintain more modern technologies, and entire industries have arisen to develop succeeding generations of increasingly more complex tools. Modern technology increasingly relies on training and education – their designers, builders, maintainers, and users often require sophisticated general and specific training.[61] Moreover, these technologies have become so complex that entire fields have developed to support them, including engineering, medicine, and computer science; and other fields have become more complex, such as construction, transportation, and architecture.

Technological change is the largest cause of long-term economic growth.[62][63] Throughout human history, energy production was the main constraint on economic development, and new technologies allowed humans to significantly increase the amount of available energy. First came fire, which made edible a wider variety of foods, and made it less physically demanding to digest them. Fire also enabled smelting, and the use of tin, copper, and iron tools, used for hunting or tradesmanship. Then came the agricultural revolution: humans no longer needed to hunt or gather to survive, and began to settle in towns and cities, forming more complex societies, with militaries and more organized forms of religion.[64]

Technologies have contributed to human welfare through increased prosperity, improved comfort and quality of life, and medical progress, but they can also disrupt existing social hierarchies, cause pollution, and harm individuals or groups.

Recent years have brought about a rise in social media's cultural prominence, with potential repercussions on democracy, and economic and social life. Early on, the internet was seen as a ""liberation technology"" that would democratize knowledge, improve access to education, and promote democracy. Modern research has turned to investigate the internet's downsides, including disinformation, polarization, hate speech, and propaganda.[65]

Since the 1970s, technology's impact on the environment has been criticized, leading to a surge in investment in solar, wind, and other forms of clean energy.

Since the invention of the wheel, technologies have helped increase humans' economic output. Past automation has both substituted and complemented labor; machines replaced humans at some lower-paying jobs (for example in agriculture), but this was compensated by the creation of new, higher-paying jobs.[66] Studies have found that computers did not create significant net technological unemployment.[67] Due to artificial intelligence being far more capable than computers, and still being in its infancy, it is not known whether it will follow the same trend; the question has been debated at length among economists and policymakers. A 2017 survey found no clear consensus among economists on whether AI would increase long-term unemployment.[68] According to the World Economic Forum's ""The Future of Jobs Report 2020"", AI is predicted to replace 85 million jobs worldwide, and create 97 million new jobs by 2025.[69][70] From 1990 to 2007, a study in the U.S. by MIT economist Daron Acemoglu showed that an addition of one robot for every 1,000 workers decreased the employment-to-population ratio by 0.2%, or about 3.3 workers, and lowered wages by 0.42%.[71][72] Concerns about technology replacing human labor however are long-lasting. As US president Lyndon Johnson said in 1964, ""Technology is creating both new opportunities and new obligations for us, opportunity for greater productivity and progress; obligation to be sure that no workingman, no family must pay an unjust price for progress."" upon signing the National Commission on Technology, Automation, and Economic Progress bill.[73][74][75][76][77]

With the growing reliance of technology, there have been security and privacy concerns along with it. Billions of people use different online payment methods, such as WeChat Pay, PayPal, Alipay, and much more to help transfer money. Although security measures are placed, some criminals are able to bypass them.[78] In March 2022, North Korea used Blender.io, a mixer which helped them to hide their cryptocurrency exchanges, to launder over $20.5 million in cryptocurrency, from Axie Infinity, and steal over $600 million worth of cryptocurrency from the game's owner. Because of this, the U.S. Treasury Department sanctioned Blender.io, which marked the first time it has taken action against a mixer, to try to crack down on North Korean hackers.[79][80] The privacy of cryptocurrency has been debated. Although many customers like the privacy of cryptocurrency, many also argue that it needs more transparency and stability.[78]

Technology can have both positive and negative effects on the environment. Environmental technology, describes an array of technologies which seek to reverse, mitigate or halt environmental damage to the environment. This can include measures to halt pollution through environmental regulations, capture and storage of pollution, or using pollutant byproducts in other industries.[81] Other examples of environmental technology include deforestation and the reversing of deforestation.[82] Emerging technologies in the fields of climate engineering may be able to halt or reverse global warming and its environmental impacts,[83] although this remains highly controversial.[84]  As technology has advanced, so too has the negative environmental impact, with increased release of greenhouse gases, including methane, nitrous oxide and carbon dioxide, into the atmosphere, causing the greenhouse effect. This continues to gradually heat the earth, causing global warming and climate change. Measures of technological innovation correlates with a rise in greenhouse gas emissions.[85]

Pollution, the presence of contaminants in an environment that causes adverse effects, could have been present as early as the Inca Empire. They used a lead sulfide flux in the smelting of ores, along with the use of a wind-drafted clay kiln, which released lead into the atmosphere and the sediment of rivers.[86]

Philosophy of technology is a branch of philosophy that studies the ""practice of designing and creating artifacts"", and the ""nature of the things so created.""[87] It emerged as a discipline over the past two centuries, and has grown ""considerably"" since the 1970s.[88] The humanities philosophy of technology is concerned with the ""meaning of technology for, and its impact on, society and culture"".[87]

Initially, technology was seen as an extension of the human organism that replicated or amplified bodily and mental faculties.[89] Marx framed it as a tool used by capitalists to oppress the proletariat, but believed that technology would be a fundamentally liberating force once it was ""freed from societal deformations"". Second-wave philosophers like Ortega later shifted their focus from economics and politics to ""daily life and living in a techno-material culture"", arguing that technology could oppress ""even the members of the bourgeoisie who were its ostensible masters and possessors."" Third-stage philosophers like Don Ihde and Albert Borgmann represent a turn toward de-generalization and empiricism, and considered how humans can learn to live with technology.[88][page needed]

Early scholarship on technology was split between two arguments: technological determinism, and social construction. Technological determinism is the idea that technologies cause unavoidable social changes.[90]: 95  It usually encompasses a related argument, technological autonomy, which asserts that technological progress follows a natural progression and cannot be prevented.[91] Social constructivists[who?] argue that technologies follow no natural progression, and are shaped by cultural values, laws, politics, and economic incentives. Modern scholarship has shifted towards an analysis of sociotechnical systems, ""assemblages of things, people, practices, and meanings"", looking at the value judgments that shape technology.[90][page needed]

Cultural critic Neil Postman distinguished tool-using societies from technological societies and from what he called ""technopolies"", societies that are dominated by an ideology of technological and scientific progress to the detriment of other cultural practices, values, and world views.[92] Herbert Marcuse and John Zerzan suggest that technological society will inevitably deprive us of our freedom and psychological health.[93]

The ethics of technology is an interdisciplinary subfield of ethics that analyzes technology's ethical implications and explores ways to mitigate potential negative impacts of new technologies. There is a broad range of ethical issues revolving around technology, from specific areas of focus affecting professionals working with technology to broader social, ethical, and legal issues concerning the role of technology in society and everyday life.[94]

Prominent debates have surrounded genetically modified organisms, the use of robotic soldiers, algorithmic bias, and the issue of aligning AI behavior with human values.[95]

Technology ethics encompasses several key fields: Bioethics looks at ethical issues surrounding biotechnologies and modern medicine, including cloning, human genetic engineering, and stem cell research. Computer ethics focuses on issues related to computing. Cyberethics explores internet-related issues like intellectual property rights, privacy, and censorship. Nanoethics examines issues surrounding the alteration of matter at the atomic and molecular level in various disciplines including computer science, engineering, and biology. And engineering ethics deals with the professional standards of engineers, including software engineers and their moral responsibilities to the public.[96]

A wide branch of technology ethics is concerned with the ethics of artificial intelligence: it includes robot ethics, which deals with ethical issues involved in the design, construction, use, and treatment of robots,[97] as well as machine ethics, which is concerned with ensuring the ethical behavior of artificially intelligent agents.[98] Within the field of AI ethics, significant yet-unsolved research problems include AI alignment (ensuring that AI behaviors are aligned with their creators' intended goals and interests) and the reduction of algorithmic bias. Some researchers have warned against the hypothetical risk of an AI takeover, and have advocated for the use of AI capability control in addition to AI alignment methods.

Other fields of ethics have had to contend with technology-related issues, including military ethics, media ethics, and educational ethics.

Futures studies is the study of social and technological progress. It aims to explore the range of plausible futures and incorporate human values in the development of new technologies.[99]: 54  More generally, futures researchers are interested in improving ""the freedom and welfare of humankind"".[99]: 73  It relies on a thorough quantitative and qualitative analysis of past and present technological trends, and attempts to rigorously extrapolate them into the future.[99] Science fiction is often used as a source of ideas.[99]: 173  Futures research methodologies include survey research, modeling, statistical analysis, and computer simulations.[99]: 187 

Existential risk researchers analyze risks that could lead to human extinction or civilizational collapse, and look for ways to build resilience against them.[100][101] Relevant research centers include the Cambridge Center for the Study of Existential Risk, and the Stanford Existential Risk Initiative.[102] Future technologies may contribute to the risks of artificial general intelligence, biological warfare, nuclear warfare, nanotechnology, anthropogenic climate change, global warming, or stable global totalitarianism, though technologies may also help us mitigate asteroid impacts and gamma-ray bursts.[103] In 2019 philosopher Nick Bostrom introduced the notion of a vulnerable world, ""one in which there is some level of technological development at which civilization almost certainly gets devastated by default"", citing the risks of a pandemic caused by bioterrorists, or an arms race triggered by the development of novel armaments and the loss of mutual assured destruction.[104] He invites policymakers to question the assumptions that technological progress is always beneficial, that scientific openness is always preferable, or that they can afford to wait until a dangerous technology has been invented before they prepare mitigations.[104]

Emerging technologies are novel technologies whose development or practical applications are still largely unrealized. They include nanotechnology, biotechnology, robotics, 3D printing, and blockchains.

In 2005, futurist Ray Kurzweil claimed the next technological revolution would rest upon advances in genetics, nanotechnology, and robotics, with robotics being the most impactful of the three technologies.[105] Genetic engineering will allow far greater control over human biological nature through a process called directed evolution. Some thinkers believe that this may shatter our sense of self, and have urged for renewed public debate exploring the issue more thoroughly;[106] others fear that directed evolution could lead to eugenics or extreme social inequality. Nanotechnology will grant us the ability to manipulate matter ""at the molecular and atomic scale"",[107] which could allow us to reshape ourselves and our environment in fundamental ways.[108] Nanobots could be used within the human body to destroy cancer cells or form new body parts, blurring the line between biology and technology.[109] Autonomous robots have undergone rapid progress, and are expected to replace humans at many dangerous tasks, including search and rescue, bomb disposal, firefighting, and war.[110]

Estimates on the advent of artificial general intelligence vary, but half of machine learning experts surveyed in 2018 believe that AI will ""accomplish every task better and more cheaply"" than humans by 2063, and automate all human jobs by 2140.[111] This expected technological unemployment has led to calls for increased emphasis on computer science education and debates about universal basic income. Political science experts predict that this could lead to a rise in extremism, while others see it as an opportunity to usher in a post-scarcity economy.

Some segments of the 1960s hippie counterculture grew to dislike urban living and developed a preference for locally autonomous, sustainable, and decentralized technology, termed appropriate technology. This later influenced hacker culture and technopaganism.

Technological utopianism refers to the belief that technological development is a moral good, which can and should bring about a utopia, that is, a society in which laws, governments, and social conditions serve the needs of all its citizens.[112] Examples of techno-utopian goals include post-scarcity economics, life extension, mind uploading, cryonics, and the creation of artificial superintelligence. Major techno-utopian movements include transhumanism and singularitarianism.

The transhumanism movement is founded upon the ""continued evolution of human life beyond its current human form"" through science and technology, informed by ""life-promoting principles and values.""[113] The movement gained wider popularity in the early 21st century.[114]

Singularitarians believe that machine superintelligence will ""accelerate technological progress"" by orders of magnitude and ""create even more intelligent entities ever faster"", which may lead to a pace of societal and technological change that is ""incomprehensible"" to us. This event horizon is known as the technological singularity.[115]

Major figures of techno-utopianism include Ray Kurzweil and Nick Bostrom. Techno-utopianism has attracted both praise and criticism from progressive, religious, and conservative thinkers.[116]

Technology's central role in our lives has drawn concerns and backlash. The backlash against technology is not a uniform movement and encompasses many heterogeneous ideologies.[117]

The earliest known revolt against technology was Luddism, a pushback against early automation in textile production. Automation had resulted in a need for fewer workers, a process known as technological unemployment.

Between the 1970s and 1990s, American terrorist Ted Kaczynski carried out a series of bombings across America and published the Unabomber Manifesto denouncing technology's negative impacts on nature and human freedom. The essay resonated with a large part of the American public.[118] It was partly inspired by Jacques Ellul's The Technological Society.[119]

Some subcultures, like the off-the-grid movement, advocate a withdrawal from technology and a return to nature. The ecovillage movement seeks to reestablish harmony between technology and nature.[120]

Engineering is the process by which technology is developed. It often requires problem-solving under strict constraints.[121] Technological development is ""action-oriented"", while scientific knowledge is fundamentally explanatory.[122] Polish philosopher Henryk Skolimowski framed it like so: ""science concerns itself with what is, technology with what is to be.""[123]: 375 

The direction of causality between scientific discovery and technological innovation has been debated by scientists, philosophers and policymakers.[124] Because innovation is often undertaken at the edge of scientific knowledge, most technologies are not derived from scientific knowledge, but instead from engineering, tinkering and chance.[125]: 217–240  For example, in the 1940s and 1950s, when knowledge of turbulent combustion or fluid dynamics was still crude, jet engines were invented through ""running the device to destruction, analyzing what broke [...] and repeating the process"".[121] Scientific explanations often follow technological developments rather than preceding them.[125]: 217–240  Many discoveries also arose from pure chance, like the discovery of penicillin as a result of accidental lab contamination.[126] Since the 1960s, the assumption that government funding of basic research would lead to the discovery of marketable technologies has lost credibility.[127][128] Probabilist Nassim Taleb argues that national research programs that implement the notions of serendipity and convexity through frequent trial and error are more likely to lead to useful innovations than research that aims to reach specific outcomes.[125][129]

Despite this, modern technology is increasingly reliant on deep, domain-specific scientific knowledge. In 1975, there was an average of one citation of scientific literature in every three patents granted in the U.S.; by 1989, this increased to an average of one citation per patent. The average was skewed upwards by patents related to the pharmaceutical industry, chemistry, and electronics.[130] A 2021 analysis shows that patents that are based on scientific discoveries are on average 26% more valuable than equivalent non-science-based patents.[131]

The use of basic technology is also a feature of non-human animal species. Tool use was once considered a defining characteristic of the genus Homo.[132] This view was supplanted after discovering evidence of tool use among chimpanzees and other primates,[133] dolphins,[134] and crows.[135][136] For example, researchers have observed wild chimpanzees using basic foraging tools, pestles, levers, using leaves as sponges, and tree bark or vines as probes to fish termites.[137] West African chimpanzees use stone hammers and anvils for cracking nuts,[138] as do capuchin monkeys of Boa Vista, Brazil.[139] Tool use is not the only form of animal technology use; for example, beaver dams, built with wooden sticks or large stones, are a technology with ""dramatic"" impacts on river habitats and ecosystems.[140]

The relationship of humanity with technology has been explored in science-fiction literature, for example in Brave New World, A Clockwork Orange, Nineteen Eighty-Four, Isaac Asimov's essays, and movies like Minority Report, Total Recall, Gattaca, and Inception. It has spawned the dystopian and futuristic cyberpunk genre, which juxtaposes futuristic technology with societal collapse, dystopia or decay.[141] Notable cyberpunk works include William Gibson's Neuromancer novel, and movies like Blade Runner, and The Matrix.
"
Engineering,"

Engineering is the practice of using natural science, mathematics, and the engineering design process[1] to solve technical problems, increase efficiency and productivity, and improve systems. Modern engineering comprises many subfields which include designing and improving infrastructure, machinery, vehicles, electronics, materials, and energy systems.[2]

The discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied mathematics, applied science, and types of application. See glossary of engineering.

The term engineering is derived from the Latin ingenium, meaning ""cleverness"".[3]

The American Engineers' Council for Professional Development (ECPD, the predecessor of ABET)[4] has defined ""engineering"" as:

The creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation and safety to life and property.[5][6]
Engineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley, etc.

The term engineering is derived from the word engineer, which itself dates back to the 14th century when an engine'er (literally, one who builds or operates a siege engine) referred to ""a constructor of military engines"".[7] In this context, now obsolete, an ""engine"" referred to a military machine, i.e., a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, e.g., the U.S. Army Corps of Engineers.

The word ""engine"" itself is of even older origin, ultimately deriving from the Latin ingenium (c. 1250), meaning ""innate quality, especially mental power, hence a clever invention.""[8]

Later, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering[6] entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering.

The pyramids in ancient Egypt, ziggurats of Mesopotamia, the Acropolis and Parthenon in Greece, the Roman aqueducts, Via Appia and Colosseum, Teotihuacán, and the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon and the Pharos of Alexandria, were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World.

The six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times.[9] The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC.[10] The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale,[11] and to move large objects in ancient Egyptian technology.[12] The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia c. 3000 BC,[11] and then in ancient Egyptian technology c. 2000 BC.[13] The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC,[14] and ancient Egypt during the Twelfth Dynasty (1991–1802 BC).[15] The screw, the last of the simple machines to be invented,[16] first appeared in Mesopotamia during the Neo-Assyrian period (911–609) BC.[14] The Egyptian pyramids were built using three of the six simple machines, the inclined plane, the wedge, and the lever, to create structures like the Great Pyramid of Giza.[17]

The earliest civil engineer known by name is Imhotep.[6] As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630–2611 BC.[18] The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC.[19]

Kush developed the Sakia during the 4th century BC, which relied on animal power instead of human energy.[20] Hafirs were developed as a type of reservoir in Kush to store and contain water as well as boost irrigation.[21] Sappers were employed to build causeways during military campaigns.[22] Kushite ancestors built speos during the Bronze Age between 3700 and 3250 BC.[23] Bloomeries and blast furnaces were also created during the 7th centuries BC in Kush.[24][25][26][27]

Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, an early known mechanical analog computer,[28][29] and the mechanical inventions of Archimedes, are examples of Greek mechanical engineering. Some of Archimedes' inventions, as well as the Antikythera mechanism, required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are widely used in fields such as robotics and automotive engineering.[30]

Ancient Chinese, Greek, Roman and Hunnic armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century BC,[31] the trireme, the ballista and the catapult. In the Middle Ages, the trebuchet was developed.

The earliest practical wind-powered machines, the windmill and wind pump, first appeared in the Muslim world during the Islamic Golden Age, in what are now Iran, Afghanistan, and Pakistan, by the 9th century AD.[32][33][34][35] The earliest practical steam-powered machine was a steam jack driven by a steam turbine, described in 1551 by Taqi al-Din Muhammad ibn Ma'ruf in Ottoman Egypt.[36][37]

The cotton gin was invented in India by the 6th century AD,[38] and the spinning wheel was invented in the Islamic world by the early 11th century,[39] both of which were fundamental to the growth of the cotton industry. The spinning wheel was also a precursor to the spinning jenny, which was a key development during the early Industrial Revolution in the 18th century.[40]

The earliest programmable machines were developed in the Muslim world. A music sequencer, a programmable musical instrument, was the earliest type of programmable machine. The first music sequencer was an automated flute player invented by the Banu Musa brothers, described in their Book of Ingenious Devices, in the 9th century.[41][42] In 1206, Al-Jazari invented programmable automata/robots. He described four automaton musicians, including drummers operated by a programmable drum machine, where they could be made to play different rhythms and different drum patterns.[43]

Before the development of modern engineering, mathematics was used by artisans and craftsmen, such as millwrights, clockmakers, instrument makers and surveyors. Aside from these professions, universities were not believed to have had much practical significance to technology.[44]: 32 

A standard reference for the state of mechanical arts during the Renaissance is given in the mining engineering treatise De re metallica (1556), which also contains sections on geology, mining, and chemistry. De re metallica was the standard chemistry reference for the next 180 years.[44]

The science of classical mechanics, sometimes called Newtonian mechanics, formed the scientific basis of much of modern engineering.[44] With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering, the fields then known as the mechanic arts became incorporated into engineering.

Canal building was an important engineering work during the early phases of the Industrial Revolution.[45]

John Smeaton was the first self-proclaimed civil engineer and is often regarded as the ""father"" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbors, and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Using a model water wheel, Smeaton conducted experiments for seven years, determining ways to increase efficiency.[46]: 127    Smeaton introduced iron axles and gears to water wheels.[44]: 69  Smeaton also made mechanical improvements to the Newcomen steam engine. Smeaton designed the third Eddystone Lighthouse (1755–59) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain ""hydraulicity"" in lime; work which led ultimately to the invention of Portland cement.

Applied science led to the development of the steam engine. The sequence of events began with the invention of the barometer and the measurement of atmospheric pressure by Evangelista Torricelli in 1643, demonstration of the force of atmospheric pressure by Otto von Guericke using the Magdeburg hemispheres in 1656, laboratory experiments by Denis Papin, who built experimental model steam engines and demonstrated the use of a piston, which he published in 1707. Edward Somerset, 2nd Marquess of Worcester published a book of 100 inventions containing a method for raising waters similar to a coffee percolator. Samuel Morland, a mathematician and inventor who worked on pumps, left notes at the Vauxhall Ordinance Office on a steam pump design that Thomas Savery read. In 1698 Savery built a steam pump called ""The Miner's Friend"". It employed both vacuum and pressure.[47] Iron merchant Thomas Newcomen, who built the first commercial piston steam engine in 1712, was not known to have any scientific training.[46]: 32 

The application of steam-powered cast iron blowing cylinders for providing pressurized air for blast furnaces lead to a large increase in iron production in the late 18th century. The higher furnace temperatures made possible with steam-powered blast allowed for the use of more lime in blast furnaces, which enabled the transition from charcoal to coke.[48] These innovations lowered the cost of iron, making horse railways and iron bridges practical. The puddling process, patented by Henry Cort in 1784 produced large scale quantities of wrought iron. Hot blast, patented by James Beaumont Neilson in 1828, greatly lowered the amount of fuel needed to smelt iron. With the development of the high pressure steam engine, the power to weight ratio of steam engines made practical steamboats and locomotives possible.[49] New steel making processes, such as the Bessemer process and the open hearth furnace, ushered in an area of heavy engineering in the late 19th century.

One of the most famous engineers of the mid-19th century was Isambard Kingdom Brunel, who built railroads, dockyards and steamships.

The Industrial Revolution created a demand for machinery with metal parts, which led to the development of several machine tools. Boring cast iron cylinders with precision was not possible until John Wilkinson invented his boring machine, which is considered the first machine tool.[50] Other machine tools included the screw cutting lathe, milling machine, turret lathe and the metal planer. Precision machining techniques were developed in the first half of the 19th century. These included the use of gigs to guide the machining tool over the work and fixtures to hold the work in the proper position. Machine tools and machining techniques capable of producing interchangeable parts lead to large scale factory production by the late 19th century.[51]

The United States Census of 1850 listed the occupation of ""engineer"" for the first time with a count of 2,000.[52] There were fewer than 50 engineering graduates in the U.S. before 1865. In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875. In 1890, there were 6,000 engineers in civil, mining, mechanical and electrical.[49]

There was no chair of applied mechanism and applied mechanics at Cambridge until 1875, and no chair of engineering at Oxford until 1907. Germany established technical universities earlier.[53]

The foundations of electrical engineering in the 1800s included the experiments of Alessandro Volta, Michael Faraday, Georg Ohm and others and the invention of the electric telegraph in 1816 and the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.[6]
Chemical engineering developed in the late nineteenth century.[6] Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants.[6] The role of the chemical engineer was the design of these chemical plants and processes.[6]

Aeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.[54]

The first PhD in engineering (technically, applied science and engineering) awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.[55]

Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I. Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.

Engineering is a broad discipline that is often broken down into several sub-disciplines. Although an engineer will usually be trained in a specific discipline, he or she may become multi-disciplined through experience. Engineering is often characterized as having four main branches:[56][57][58] chemical engineering, civil engineering, electrical engineering, and mechanical engineering.

Chemical engineering is the application of physics, chemistry, biology, and engineering principles in order to carry out chemical processes on a commercial scale, such as the manufacture of commodity chemicals, specialty chemicals, petroleum refining, microfabrication, fermentation, and biomolecule production.

Civil engineering is the design and construction of public and private works, such as infrastructure (airports, roads, railways, water supply, and treatment etc.), bridges, tunnels, dams, and buildings.[59][60] Civil engineering is traditionally broken into a number of sub-disciplines, including structural engineering, environmental engineering, and surveying. It is traditionally considered to be separate from military engineering.[61]

Electrical engineering is the design, study, and manufacture of various electrical and electronic systems, such as broadcast engineering, electrical circuits, generators, motors, electromagnetic/electromechanical devices, electronic devices, electronic circuits, optical fibers, optoelectronic devices, computer systems, telecommunications, instrumentation, control systems, and electronics.

Mechanical engineering is the design and manufacture of physical or mechanical systems, such as power and energy systems, aerospace/aircraft products, weapon systems, transportation products, engines, compressors, powertrains, kinematic chains, vacuum technology, vibration isolation equipment, manufacturing, robotics, turbines, audio equipments, and mechatronics.

Bioengineering is the engineering of biological systems for a useful purpose. Examples of bioengineering research include bacteria engineered to produce chemicals, new medical imaging technology, portable and rapid disease diagnostic devices, prosthetics, biopharmaceuticals, and tissue-engineered organs.

Interdisciplinary engineering draws from more than one of the principle branches of the practice. Historically, naval engineering and mining engineering were major branches. Other engineering fields are manufacturing engineering, acoustical engineering, corrosion engineering, instrumentation and control, aerospace, automotive, computer, electronic, information engineering, petroleum, environmental, systems, audio, software, architectural, agricultural, biosystems, biomedical,[62] geological, textile, industrial, materials,[63] and nuclear engineering.[64] These and other branches of engineering are represented in the 36 licensed member institutions of the UK Engineering Council.

New specialties sometimes combine with the traditional fields and form new branches – for example, Earth systems engineering and management involves a wide range of subject areas including engineering studies, environmental science, engineering ethics and philosophy of engineering.

Aerospace engineering covers the design, development, manufacture and operational behaviour of aircraft, satellites and rockets.

Marine engineering covers the design, development, manufacture and operational behaviour of watercraft and stationary structures like oil platforms and ports.

Computer engineering (CE) is a branch of engineering that integrates several fields of computer science and electronic engineering required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering.

Geological engineering is associated with anything constructed on or within the Earth. This discipline applies geological sciences and engineering principles to direct or support the work of other disciplines such as civil engineering, environmental engineering, and mining engineering. Geological engineers are involved with impact studies for facilities and operations that affect surface and subsurface environments, such as rock excavations (e.g. tunnels), building foundation consolidation, slope and fill stabilization, landslide risk assessment, groundwater monitoring, groundwater remediation, mining excavations, and natural resource exploration.

One who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur, European Engineer, or Designated Engineering Representative.

In the engineering design process, engineers apply mathematics and sciences such as physics to find novel solutions to problems or to improve existing solutions. Engineers need proficient knowledge of relevant sciences for their design projects. As a result, many engineers continue to learn new material throughout their careers.

If multiple solutions exist, engineers weigh each design choice based on their merit and choose the solution that best matches the requirements. The task of the engineer is to identify, understand, and interpret the constraints on a design in order to yield a successful result. It is generally insufficient to build a technically successful product, rather, it must also meet further requirements.

Constraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.

Engineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a particular problem. Creating an appropriate mathematical model of a problem often allows them to analyze it (sometimes definitively), and to test potential solutions.[65]

More than one solution to a design problem usually exists so the different design choices have to be evaluated on their merits before the one judged most suitable is chosen. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of ""low-level"" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.[66]

Engineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected but only in so far as the testing has been representative of use in service. For products, such as aircraft, that are used differently by different users failures and unexpected shortcomings (and necessary design changes) can be expected throughout the operational life of the product.[67]

Engineers take on the responsibility of producing designs that will perform as well as expected and, except those employed in specific areas of the arms industry, will not harm people. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure.

The study of failed products is known as forensic engineering. It attempts to identify the cause of failure to allow a redesign of the product and so prevent a re-occurrence. Careful analysis is needed to establish the cause of failure of a product. The consequences of a failure may vary in severity from the minor cost of a machine breakdown to large loss of life in the case of accidents involving aircraft and large stationary structures like buildings and dams.[68]

As with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.

One of the most widely used design tools in the profession is computer-aided design (CAD) software. It enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.

These allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.[69]

There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and Architecture, engineering and construction (AEC) software for civil engineering.

In recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).[70]

The engineering profession engages in a range of activities, from collaboration at the societal level, and smaller individual projects. Almost all engineering projects are obligated to a funding source: a company, a set of investors, or a government. The types of engineering that are less constrained by such a funding source, are pro bono, and open-design engineering.

Engineering has interconnections with society, culture and human behavior. Most products and constructions used by modern society, are influenced by engineering. Engineering activities have an impact on the environment, society, economies, and public safety.

Engineering projects can be controversial. Examples from different engineering disciplines include: the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some engineering companies have enacted serious corporate and social responsibility policies.

The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.[71]

Overseas development and relief NGOs make considerable use of engineers, to apply solutions in disaster and development scenarios. Some charitable organizations use engineering directly for development:

Engineering companies in more developed economies face challenges with regard to the number of engineers being trained, compared with those retiring. This problem is prominent in the UK where engineering has a poor image and low status.[73] There are negative economic and political issues that this can cause, as well as ethical issues.[74] It is agreed the engineering profession faces an ""image crisis"".[75] The UK holds the most engineering companies compared to other European countries, together with the United States.[citation needed]

Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large. The National Society of Professional Engineers code of ethics states:

 Engineering is an important and learned profession. As members of this profession, engineers are expected to exhibit the highest standards of honesty and integrity. Engineering has a direct and vital impact on the quality of life for all people. Accordingly, the services provided by engineers require honesty, impartiality, fairness, and equity, and must be dedicated to the protection of the public health, safety, and welfare. Engineers must perform under a standard of professional behavior that requires adherence to the highest principles of ethical conduct.[76]
In Canada, engineers wear the Iron Ring as a symbol and reminder of the obligations and ethics associated with their profession.[77]

Scientists study the world as it is; engineers create the world that has never been.
There exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations.[citation needed]

Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology, engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely ""engineering scientists"".[81]

In the book What Engineers Know and How They Know It,[82] Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner.

There is a ""real and important"" difference between engineering and physics as similar to any science field has to do with technology.[83][84] Physics is an exploratory science that seeks knowledge of principles while engineering uses knowledge for practical applications of principles. The former equates an understanding into a mathematical principle while the latter measures variables involved and creates technology.[85][86][87] For technology, physics is an auxiliary and in a way technology is considered as applied physics.[88] Though physics and engineering are interrelated, it does not mean that a physicist is trained to do an engineer's job. A physicist would typically require additional and relevant training.[89] Physicists and engineers engage in different lines of work.[90] But PhD physicists who specialize in sectors of engineering physics and applied physics are titled as Technology officer, R&D Engineers and System Engineers.[91]

An example of this is the use of numerical approximations to the Navier–Stokes equations to describe aerodynamic flow over an aircraft, or the use of the finite element method to calculate the stresses in complex components. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.[92]

As stated by Fung et al. in the revision to the classic engineering text Foundations of Solid Mechanics:

Engineering is quite different from science. Scientists try to understand nature. Engineers try to make things that do not exist in nature. Engineers stress innovation and invention. To embody an invention the engineer must put his idea in concrete terms, and design something that people can use. That something can be a complex system, device, a gadget, a material, a method, a computing program, an innovative experiment, a new solution to a problem, or an improvement on what already exists. Since a design has to be realistic and functional, it must have its geometry, dimensions, and characteristics data defined. In the past engineers working on new designs found that they did not have all the required information to make design decisions. Most often, they were limited by insufficient scientific knowledge. Thus they studied mathematics, physics, chemistry, biology and mechanics. Often they had to add to the sciences relevant to their profession. Thus engineering sciences were born.[93]
Although engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability, and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution.[94]

The study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology.

Modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers.[95][96] The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems.

Conversely, some engineering disciplines view the human body as a biological machine worth studying and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine.[97][98]

Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.

Medicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.[99]

The heart for example functions much like a pump,[100] the skeleton is like a linked structure with levers,[101] the brain produces electrical signals etc.[102] These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.

Newly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.[99]

There are connections between engineering and art, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering).[104][105][106]

The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design.[107] Robert Maillart's bridge design is perceived by some to have been deliberately artistic.[108] At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.[104][109]

Among famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.[103][110]

Business engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management or ""Management engineering"" is a specialized field of management concerned with engineering practice or the engineering industry sector. The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge, and expertise, alongside knowledge of business administration, management techniques, and strategic thinking. Engineers specializing in change management must have in-depth knowledge of the application of industrial and organizational psychology principles and methods. Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to engineering practice or the engineering sector. This work often deals with large scale complex business transformation or business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical and electronics, power distribution and generation, utilities and transportation systems. This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives.

In political science, the term engineering has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Marketing engineering and financial engineering have similarly borrowed the term.
"
Mathematics,"

Mathematics is a field of study that discovers and organizes methods, theories and theorems that are developed and proved for the needs of empirical sciences and mathematics itself. There are many areas of mathematics, which include number theory (the study of numbers), algebra (the study of formulas and related structures), geometry (the study of shapes and spaces that contain them), analysis (the study of continuous changes), and set theory (presently used as a foundation for all mathematics).

Mathematics involves the description and manipulation of abstract objects that consist of either abstractions from nature or—in modern mathematics—purely abstract entities that are stipulated to have certain properties, called axioms.  Mathematics uses pure reason to prove properties of objects, a proof consisting of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered true starting points of the theory under consideration.[1]

Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science, and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent of any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics) but often later find practical applications.[2][3]

Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements.[4] Since its beginning, mathematics was primarily divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra[a] and infinitesimal calculus were introduced as new fields. Since then, the interaction between mathematical innovations and scientific discoveries has led to a correlated increase in the development of both.[5] At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method,[6] which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than sixty first-level areas of mathematics.


Before the Renaissance, mathematics was divided into two main areas: arithmetic, regarding the manipulation of numbers, and geometry, regarding the study of shapes.[7] Some types of pseudoscience, such as numerology and astrology, were not then clearly distinguished from mathematics.[8]

During the Renaissance, two more areas appeared. Mathematical notation led to algebra which, roughly speaking, consists of the study and the manipulation of formulas. Calculus, consisting of the two subfields differential calculus and integral calculus, is the study of continuous functions, which model the typically nonlinear relationships between varying quantities, as represented by variables. This division into four main areas—arithmetic, geometry, algebra, and calculus[9]—endured until the end of the 19th century. Areas such as celestial mechanics and solid mechanics were then studied by mathematicians, but now are considered as belonging to physics.[10] The subject of combinatorics has been studied for much of recorded history, yet did not become a separate branch of mathematics until the seventeenth century.[11]

At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics.[12][6] The 2020 Mathematics Subject Classification contains no less than sixty-three first-level areas.[13] Some of these areas correspond to the older division, as is true regarding number theory (the modern name for higher arithmetic) and geometry. Several other first-level areas have ""geometry"" in their names or are otherwise commonly considered part of geometry. Algebra and calculus do not appear as first-level areas but are respectively split into several first-level areas. Other first-level areas emerged during the 20th century or had not previously been considered as mathematics, such as mathematical logic and foundations.[14]

Number theory began with the manipulation of numbers, that is, natural numbers 



(

N

)
,


{\displaystyle (\mathbb {N} ),}

 and later expanded to integers 



(

Z

)


{\displaystyle (\mathbb {Z} )}

 and rational numbers 



(

Q

)
.


{\displaystyle (\mathbb {Q} ).}

 Number theory was once called arithmetic, but nowadays this term is mostly used for numerical calculations.[15] Number theory dates back to ancient Babylon and probably China. Two prominent early number theorists were Euclid of ancient Greece and Diophantus of Alexandria.[16] The modern study of number theory in its abstract form is largely attributed to Pierre de Fermat and Leonhard Euler. The field came to full fruition with the contributions of Adrien-Marie Legendre and Carl Friedrich Gauss.[17]

Many easily stated number problems have solutions that require sophisticated methods, often from across mathematics. A prominent example is Fermat's Last Theorem. This conjecture was stated in 1637 by Pierre de Fermat, but it was proved only in 1994 by Andrew Wiles, who used tools including scheme theory from algebraic geometry, category theory, and homological algebra.[18] Another example is Goldbach's conjecture, which asserts that every even integer greater than 2 is the sum of two prime numbers. Stated in 1742 by Christian Goldbach, it remains unproven despite considerable effort.[19]

Number theory includes several subareas, including analytic number theory, algebraic number theory, geometry of numbers (method oriented), diophantine equations, and transcendence theory (problem oriented).[14]

Geometry is one of the oldest branches of mathematics. It started with empirical recipes concerning shapes, such as lines, angles and circles, which were developed mainly for the needs of surveying and architecture, but has since blossomed out into many other subfields.[20]

A fundamental innovation was the ancient Greeks' introduction of the concept of proofs, which require that every assertion must be proved. For example, it is not sufficient to verify by measurement that, say, two lengths are equal; their equality must be proven via reasoning from previously accepted results (theorems) and a few basic statements. The basic statements are not subject to proof because they are self-evident (postulates), or are part of the definition of the subject of study (axioms). This principle, foundational for all mathematics, was first elaborated for geometry, and was systematized by Euclid around 300 BC in his book Elements.[21][22]

The resulting Euclidean geometry is the study of shapes and their arrangements constructed from lines, planes and circles in the Euclidean plane (plane geometry) and the three-dimensional Euclidean space.[b][20]

Euclidean geometry was developed without change of methods or scope until the 17th century, when René Descartes introduced what is now called Cartesian coordinates. This constituted a major change of paradigm: Instead of defining real numbers as lengths of line segments (see number line), it allowed the representation of points using their coordinates, which are numbers. Algebra (and later, calculus) can thus be used to solve geometrical problems. Geometry was split into two new subfields: synthetic geometry, which uses purely geometrical methods, and analytic geometry, which uses coordinates systemically.[23]

Analytic geometry allows the study of curves unrelated to circles and lines. Such curves can be defined as the graph of functions, the study of which led to differential geometry. They can also be defined as implicit equations, often polynomial equations (which spawned algebraic geometry). Analytic geometry also makes it possible to consider Euclidean spaces of higher than three dimensions.[20]

In the 19th century, mathematicians discovered non-Euclidean geometries, which do not follow the parallel postulate. By questioning that postulate's truth, this discovery has been viewed as joining Russell's paradox in revealing the foundational crisis of mathematics. This aspect of the crisis was solved by systematizing the axiomatic method, and adopting that the truth of the chosen axioms is not a mathematical problem.[24][6] In turn, the axiomatic method allows for the study of various geometries obtained either by changing the axioms or by considering properties that do not change under specific transformations of the space.[25]

Today's subareas of geometry include:[14]

Algebra is the art of manipulating equations and formulas. Diophantus (3rd century) and al-Khwarizmi (9th century) were the two main precursors of algebra.[27][28] Diophantus solved some equations involving unknown natural numbers by deducing new relations until he obtained the solution.[29] Al-Khwarizmi introduced systematic methods for transforming equations, such as moving a term from one side of an equation into the other side.[30] The term algebra is derived from the Arabic word al-jabr meaning 'the reunion of broken parts' that he used for naming one of these methods in the title of his main treatise.[31][32]

Algebra became an area in its own right only with François Viète (1540–1603), who introduced the use of variables for representing unknown or unspecified numbers.[33] Variables allow mathematicians to describe the operations that have to be done on the numbers represented using mathematical formulas.[34]

Until the 19th century, algebra consisted mainly of the study of linear equations (presently linear algebra), and polynomial equations in a single unknown, which were called algebraic equations (a term still in use, although it may be ambiguous). During the 19th century, mathematicians began to use variables to represent things other than numbers (such as matrices, modular integers, and geometric transformations), on which generalizations of arithmetic operations are often valid.[35] The concept of algebraic structure addresses this, consisting of a set whose elements are unspecified, of operations acting on the elements of the set, and rules that these operations must follow. The scope of algebra thus grew to include the study of algebraic structures. This object of algebra was called modern algebra or abstract algebra, as established by the influence and works of Emmy Noether.[36]

Some types of algebraic structures have useful and often fundamental properties, in many areas of mathematics. Their study became autonomous parts of algebra, and include:[14]

The study of types of algebraic structures as mathematical objects is the purpose of universal algebra and category theory.[37] The latter applies to every mathematical structure (not only algebraic ones). At its origin, it was introduced, together with homological algebra for allowing the algebraic study of non-algebraic objects such as topological spaces; this particular area of application is called algebraic topology.[38]

Calculus, formerly called infinitesimal calculus, was introduced independently and simultaneously by 17th-century mathematicians Newton and Leibniz.[39] It is fundamentally the study of the relationship of variables that depend on each other. Calculus was expanded in the 18th century by Euler with the introduction of the concept of a function and many other results.[40] Presently, ""calculus"" refers mainly to the elementary part of this theory, and ""analysis"" is commonly used for advanced parts.[41]

Analysis is further subdivided into real analysis, where variables represent real numbers, and complex analysis, where variables represent complex numbers. Analysis includes many subareas shared by other areas of mathematics which include:[14]

Discrete mathematics, broadly speaking, is the study of individual, countable mathematical objects. An example is the set of all integers.[42] Because the objects of study here are discrete, the methods of calculus and mathematical analysis do not directly apply.[c] Algorithms—especially their implementation and computational complexity—play a major role in discrete mathematics.[43]

The four color theorem and optimal sphere packing were two major problems of discrete mathematics solved in the second half of the 20th century.[44] The P versus NP problem, which remains open to this day, is also important for discrete mathematics, since its solution would potentially impact a large number of computationally difficult problems.[45]

Discrete mathematics includes:[14]

The two subjects of mathematical logic and set theory have belonged to mathematics since the end of the 19th century.[46][47] Before this period, sets were not considered to be mathematical objects, and logic, although used for mathematical proofs, belonged to philosophy and was not specifically studied by mathematicians.[48]

Before Cantor's study of infinite sets, mathematicians were reluctant to consider actually infinite collections, and considered infinity to be the result of endless enumeration. Cantor's work offended many mathematicians not only by considering actually infinite sets[49] but by showing that this implies different sizes of infinity, per Cantor's diagonal argument. This led to the controversy over Cantor's set theory.[50] In the same period, various areas of mathematics concluded the former intuitive definitions of the basic mathematical objects were insufficient for ensuring mathematical rigour.[51]

This became the foundational crisis of mathematics.[52] It was eventually solved in mainstream mathematics by systematizing the axiomatic method inside a formalized set theory. Roughly speaking, each mathematical object is defined by the set of all similar objects and the properties that these objects must have.[12] For example, in Peano arithmetic, the natural numbers are defined by ""zero is a number"", ""each number has a unique successor"", ""each number but zero has a unique predecessor"", and some rules of reasoning.[53] This mathematical abstraction from reality is embodied in the modern philosophy of formalism, as founded by David Hilbert around 1910.[54]

The ""nature"" of the objects defined this way is a philosophical problem that mathematicians leave to philosophers, even if many mathematicians have opinions on this nature, and use their opinion—sometimes called ""intuition""—to guide their study and proofs. The approach allows considering ""logics"" (that is, sets of allowed deducing rules), theorems, proofs, etc. as mathematical objects, and to prove theorems about them. For example, Gödel's incompleteness theorems assert, roughly speaking that, in every consistent formal system that contains the natural numbers, there are theorems that are true (that is provable in a stronger system), but not provable inside the system.[55] This approach to the foundations of mathematics was challenged during the first half of the 20th century by mathematicians led by Brouwer, who promoted intuitionistic logic, which explicitly lacks the law of excluded middle.[56][57]

These problems and debates led to a wide expansion of mathematical logic, with subareas such as model theory (modeling some logical theories inside other theories), proof theory, type theory, computability theory and computational complexity theory.[14] Although these aspects of mathematical logic were introduced before the rise of computers, their use in compiler design, formal verification, program analysis, proof assistants and other aspects of computer science, contributed in turn to the expansion of these logical theories.[58]

The field of statistics is a mathematical application that is employed for the collection and processing of data samples, using procedures based on mathematical methods especially probability theory. Statisticians generate data with random sampling or randomized experiments.[60]

Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints. For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[61] Because of its use of optimization, the mathematical theory of statistics overlaps with other decision sciences, such as operations research, control theory, and mathematical economics.[62]

Computational mathematics is the study of mathematical problems that are typically too large for human, numerical capacity.[63][64] Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis broadly includes the study of approximation and discretization with special focus on rounding errors.[65] Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic-matrix-and-graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.

The word mathematics comes from the Ancient Greek word máthēma (μάθημα), meaning 'something learned, knowledge, mathematics', and the derived expression mathēmatikḗ tékhnē (μαθηματικὴ τέχνη), meaning 'mathematical science'. It entered the English language during the Late Middle English period through French and Latin.[66]

Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant ""learners"" rather than ""mathematicians"" in the modern sense. The Pythagoreans were likely the first to constrain the use of the word to just the study of arithmetic and geometry. By the time of Aristotle (384–322 BC) this meaning was fully established.[67]

In Latin and English, until around 1700, the term mathematics more commonly meant ""astrology"" (or sometimes ""astronomy"") rather than ""mathematics""; the meaning gradually changed to its present one from about 1500 to 1800. This change has resulted in several mistranslations: For example, Saint Augustine's warning that Christians should beware of mathematici, meaning ""astrologers"", is sometimes mistranslated as a condemnation of mathematicians.[68]

The apparent plural form in English goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural ta mathēmatiká (τὰ μαθηματικά) and means roughly ""all things mathematical"", although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, inherited from Greek.[69] In English, the noun mathematics takes a singular verb. It is often shortened to maths[70] or, in North America, math.[71]

In addition to recognizing how to count physical objects, prehistoric peoples may have also known how to count abstract quantities, like time—days, seasons, or years.[72][73] Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra, and geometry for taxation and other financial calculations, for building and construction, and for astronomy.[74] The oldest mathematical texts from Mesopotamia and Egypt are from 2000 to 1800 BC.[75] Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical concept after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication, and division) first appear in the archaeological record. The Babylonians also possessed a place-value system and used a sexagesimal numeral system which is still in use today for measuring angles and time.[76]

In the 6th century BC, Greek mathematics began to emerge as a distinct discipline and some Ancient Greeks such as the Pythagoreans appeared to have considered it a subject in its own right.[77] Around 300 BC, Euclid organized mathematical knowledge by way of postulates and first principles, which evolved into the axiomatic method that is used in mathematics today, consisting of definition, axiom, theorem, and proof.[78] His book, Elements, is widely considered the most successful and influential textbook of all time.[79] The greatest mathematician of antiquity is often held to be Archimedes (c. 287 – c. 212 BC) of Syracuse.[80] He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus.[81] Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC),[82] trigonometry (Hipparchus of Nicaea, 2nd century BC),[83] and the beginnings of algebra (Diophantus, 3rd century AD).[84]

The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics.[85] Other notable developments of Indian mathematics include the modern definition and approximation of sine and cosine, and an early form of infinite series.[86][87]

During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other achievements of the Islamic period include advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system.[88] Many notable mathematicians from this period were Persian, such as Al-Khwarizmi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī.[89] The Greek and Arabic mathematical texts were in turn translated to Latin during the Middle Ages and made available in Europe.[90]

During the early modern period, mathematics began to develop at an accelerating pace in Western Europe, with innovations that revolutionized mathematics, such as the introduction of variables and symbolic notation by François Viète (1540–1603), the introduction of logarithms by John Napier in 1614, which greatly simplified numerical calculations, especially for astronomy and marine navigation, the introduction of coordinates by René Descartes (1596–1650) for reducing geometry to algebra, and the development of calculus by Isaac Newton (1643–1727) and Gottfried Leibniz (1646–1716). Leonhard Euler (1707–1783), the most notable mathematician of the 18th century, unified these innovations into a single corpus with a standardized terminology, and completed them with the discovery and the proof of numerous theorems.[91]

Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory, number theory, and statistics.[92] In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show in part that any consistent axiomatic system—if powerful enough to describe arithmetic—will contain true propositions that cannot be proved.[55]

Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made to this very day. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, ""The number of papers and books included in the Mathematical Reviews (MR) database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs.""[93]

Mathematical notation is widely used in science and engineering for representing complex concepts and properties in a concise, unambiguous, and accurate way. This notation consists of symbols used for representing operations, unspecified numbers, relations and any other mathematical objects, and then assembling them into expressions and formulas.[94] More precisely, numbers and other mathematical objects are represented by symbols called variables, which are generally Latin or Greek letters, and often include subscripts. Operation and relations are generally represented by specific symbols or glyphs,[95] such as + (plus), × (multiplication), 



∫


{\textstyle \int }

 (integral), = (equal), and < (less than).[96] All these symbols are generally grouped according to specific rules to form expressions and formulas.[97] Normally, expressions and formulas do not appear alone, but are included in sentences of the current language, where expressions play the role of noun phrases and formulas play the role of clauses.

Mathematics has developed a rich terminology covering a broad range of fields that study the properties of various abstract, idealized objects and how they interact. It is based on rigorous definitions that provide a standard foundation for communication. An axiom or postulate is a mathematical statement that is taken to be true without need of proof. If a mathematical statement has yet to be proven (or disproven), it is termed a conjecture. Through a series of rigorous arguments employing deductive reasoning, a statement that is proven to be true becomes a theorem. A specialized theorem that is mainly used to prove another theorem is called a lemma. A proven instance that forms part of a more general finding is termed a corollary.[98]

Numerous technical terms used in mathematics are neologisms, such as polynomial and homeomorphism.[99] Other technical terms are words of the common language that are used in an accurate meaning that may differ slightly from their common meaning. For example, in mathematics, ""or"" means ""one, the other or both"", while, in common language, it is either ambiguous or means ""one or the other but not both"" (in mathematics, the latter is called ""exclusive or""). Finally, many mathematical terms are common words that are used with a completely different meaning.[100] This may lead to sentences that are correct and true mathematical assertions, but appear to be nonsense to people who do not have the required background. For example, ""every free module is flat"" and ""a field is always a ring"".

Mathematics is used in most sciences for modeling phenomena, which then allows predictions to be made from experimental laws.[101] The independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model.[102] Inaccurate predictions, rather than being caused by invalid mathematical concepts, imply the need to change the mathematical model used.[103] For example, the perihelion precession of Mercury could only be explained after the emergence of Einstein's general relativity, which replaced Newton's law of gravitation as a better mathematical model.[104]

There is still a philosophical debate whether mathematics is a science. However, in practice, mathematicians are typically grouped with scientists, and mathematics shares much in common with the physical sciences. Like them, it is falsifiable, which means in mathematics that, if a result or a theory is wrong, this can be proved by providing a counterexample. Similarly as in science, theories and results (theorems) are often obtained from experimentation.[105] In mathematics, the experimentation may consist of computation on selected examples or of the study of figures or other representations of mathematical objects (often mind representations without physical support). For example, when asked how he came about his theorems, Gauss once replied ""durch planmässiges Tattonieren"" (through systematic experimentation).[106] However, some authors emphasize that mathematics differs from the modern notion of science by not relying on empirical evidence.[107][108][109][110]

Until the 19th century, the development of mathematics in the West was mainly motivated by the needs of technology and science, and there was no clear distinction between pure and applied mathematics.[111] For example, the natural numbers and arithmetic were introduced for the need of counting, and geometry was motivated by surveying, architecture and astronomy. Later, Isaac Newton introduced infinitesimal calculus for explaining the movement of the planets with his law of gravitation. Moreover, most mathematicians were also scientists, and many scientists were also mathematicians.[112] However, a notable exception occurred with the tradition of pure mathematics in Ancient Greece.[113] The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.[114]

In the 19th century, mathematicians such as Karl Weierstrass and Richard Dedekind increasingly focused their research on internal problems, that is, pure mathematics.[111][115] This led to split mathematics into pure mathematics and applied mathematics, the latter being often considered as having a lower value among mathematical purists. However, the lines between the two are frequently blurred.[116]

The aftermath of World War II led to a surge in the development of applied mathematics in the US and elsewhere.[117][118] Many of the theories developed for applications were found interesting from the point of view of pure mathematics, and many results of pure mathematics were shown to have applications outside mathematics; in turn, the study of these applications may give new insights on the ""pure theory"".[119][120]

An example of the first case is the theory of distributions, introduced by Laurent Schwartz for validating computations done in quantum mechanics, which became immediately an important tool of (pure) mathematical analysis.[121] An example of the second case is the decidability of the first-order theory of the real numbers, a problem of pure mathematics that was proved true by Alfred Tarski, with an algorithm that is impossible to implement because of a computational complexity that is much too high.[122] For getting an algorithm that can be implemented and can solve systems of polynomial equations and inequalities, George Collins introduced the cylindrical algebraic decomposition that became a fundamental tool in real algebraic geometry.[123]

In the present day, the distinction between pure and applied mathematics is more a question of personal research aim of mathematicians than a division of mathematics into broad areas.[124][125] The Mathematics Subject Classification has a section for ""general applied mathematics"" but does not mention ""pure mathematics"".[14] However, these terms are still used in names of some university departments, such as at the Faculty of Mathematics at the University of Cambridge.

The unreasonable effectiveness of mathematics is a phenomenon that was named and first made explicit by physicist Eugene Wigner.[3] It is the fact that many mathematical theories (even the ""purest"") have applications outside their initial object. These applications may be completely outside their initial area of mathematics, and may concern physical phenomena that were completely unknown when the mathematical theory was introduced.[126] Examples of unexpected applications of mathematical theories can be found in many areas of mathematics.

A notable example is the prime factorization of natural numbers that was discovered more than 2,000 years before its common use for secure internet communications through the RSA cryptosystem.[127] A second historical example is the theory of ellipses. They were studied by the ancient Greek mathematicians as conic sections (that is, intersections of cones with planes). It was almost 2,000 years later that Johannes Kepler discovered that the trajectories of the planets are ellipses.[128]

In the 19th century, the internal development of geometry (pure mathematics) led to definition and study of non-Euclidean geometries, spaces of dimension higher than three and manifolds. At this time, these concepts seemed totally disconnected from the physical reality, but at the beginning of the 20th century, Albert Einstein developed the theory of relativity that uses fundamentally these concepts. In particular, spacetime of special relativity is a non-Euclidean space of dimension four, and spacetime of general relativity is a (curved) manifold of dimension four.[129][130]

A striking aspect of the interaction between mathematics and physics is when mathematics drives research in physics. This is illustrated by the discoveries of the positron and the baryon 




Ω

−


.


{\displaystyle \Omega ^{-}.}

 In both cases, the equations of the theories had unexplained solutions, which led to conjecture of the existence of an unknown particle, and the search for these particles. In both cases, these particles were discovered a few years later by specific experiments.[131][132][133]

Mathematics and physics have influenced each other over their modern history. Modern physics uses mathematics abundantly,[134] and is also considered to be the motivation of major mathematical developments.[135]

Computing is closely related to mathematics in several ways.[136] Theoretical computer science is considered to be mathematical in nature.[137] Communication technologies apply branches of mathematics that may be very old (e.g., arithmetic), especially with respect to transmission security, in cryptography and coding theory. Discrete mathematics is useful in many areas of computer science, such as complexity theory, information theory, and graph theory.[138] In 1998, the Kepler conjecture on sphere packing seemed to also be partially proven by computer.[139]

Biology uses probability extensively in fields such as ecology or neurobiology.[140]  Most discussion of probability centers on the concept of evolutionary fitness.[140] Ecology heavily uses modeling to simulate population dynamics,[140][141] study ecosystems such as the predator-prey model, measure pollution diffusion,[142] or to assess climate change.[143] The dynamics of a population can be modeled by coupled differential equations, such as the Lotka–Volterra equations.[144]

Statistical hypothesis testing, is run on data from clinical trials to determine whether a new treatment works.[145] Since the start of the 20th century, chemistry has used computing to model molecules in three dimensions.[146]

Structural geology and climatology use probabilistic models to predict the risk of natural catastrophes.[147] Similarly, meteorology, oceanography, and planetology also use mathematics due to their heavy use of models.[148][149][150]

Areas of mathematics used in the social sciences include probability/statistics and differential equations. These are used in linguistics, economics, sociology,[151] and psychology.[152]

Often the fundamental postulate of mathematical economics is that of the rational individual actor – Homo economicus (lit. 'economic man').[153] In this model, the individual seeks to maximize their self-interest,[153] and always makes optimal choices using perfect information.[154] This atomistic view of economics allows it to relatively easily mathematize its thinking, because individual calculations are transposed into mathematical calculations. Such mathematical modeling allows one to probe economic mechanisms. Some reject or criticise the concept of Homo economicus. Economists note that real people have limited information, make poor choices and care about fairness, altruism, not just personal gain.[155]

Without mathematical modeling, it is hard to go beyond statistical observations or untestable speculation. Mathematical modeling allows economists to create structured frameworks to test hypotheses and analyze complex interactions. Models provide clarity and precision, enabling the translation of theoretical concepts into quantifiable predictions that can be tested against real-world data.[156]

At the start of the 20th century, there was a development to express historical movements in formulas. In 1922, Nikolai Kondratiev discerned the ~50-year-long Kondratiev cycle, which explains phases of economic growth or crisis.[157] Towards the end of the 19th century, mathematicians extended their analysis into geopolitics.[158] Peter Turchin developed cliodynamics since the 1990s.[159]

Mathematization of the social sciences is not without risk. In the controversial book Fashionable Nonsense (1997), Sokal and Bricmont denounced the unfounded or abusive use of scientific terminology, particularly from mathematics or physics, in the social sciences.[160] The study of complex systems (evolution of unemployment, business capital, demographic evolution of a population, etc.) uses mathematical knowledge. However, the choice of counting criteria, particularly for unemployment, or of models, can be subject to controversy.[161][162]

The connection between mathematics and material reality has led to philosophical debates since at least the time of Pythagoras. The ancient philosopher Plato argued that abstractions that reflect material reality have themselves a reality that exists outside space and time. As a result, the philosophical view that mathematical objects somehow exist on their own in abstraction is often referred to as Platonism. Independently of their possible philosophical opinions, modern mathematicians may be generally considered as Platonists, since they think of and talk of their objects of study as real objects.[163]

Armand Borel summarized this view of mathematics reality as follows, and provided quotations of G. H. Hardy, Charles Hermite, Henri Poincaré and Albert Einstein that support his views.[131]

 Something becomes objective (as opposed to ""subjective"") as soon as we are convinced that it exists in the minds of others in the same form as it does in ours and that we can think about it and discuss it together.[164] Because the language of mathematics is so precise, it is ideally suited to defining concepts for which such a consensus exists. In my opinion, that is sufficient to provide us with a feeling of an objective existence, of a reality of mathematics ...
Nevertheless, Platonism and the concurrent views on abstraction do not explain the unreasonable effectiveness of mathematics.[165]

There is no general consensus about the definition of mathematics or its epistemological status—that is, its place inside knowledge. A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. There is not even consensus on whether mathematics is an art or a science. Some just say, ""mathematics is what mathematicians do"".[166][167] A common approach is to define mathematics by its object of study.[168][169][170][171]

Aristotle defined mathematics as ""the science of quantity"" and this definition prevailed until the 18th century. However, Aristotle also noted a focus on quantity alone may not distinguish mathematics from sciences like physics; in his view, abstraction and studying quantity as a property ""separable in thought"" from real instances set mathematics apart.[172] In the 19th century, when mathematicians began to address topics—such as infinite sets—which have no clear-cut relation to physical reality, a variety of new definitions were given.[173] With the large number of new areas of mathematics that have appeared since the beginning of the 20th century, defining mathematics by its object of study has become increasingly difficult.[174] For example, in lieu of a definition, Saunders Mac Lane in Mathematics, form and function summarizes the basics of several areas of mathematics, emphasizing their inter-connectedness, and observes:[175]

the development of Mathematics provides a tightly connected network of formal rules, concepts, and systems.  Nodes of this network are closely bound to procedures useful in human activities and to questions arising in science. The transition from activities to the formal Mathematical systems is guided by a variety of general insights and ideas.
Another approach for defining mathematics is to use its methods. For example, an area of study is often qualified as mathematics as soon as one can prove theorems—assertions whose validity relies on a proof, that is, a purely-logical deduction.[d][176][failed verification]

Mathematical reasoning requires rigor. This means that the definitions must be absolutely unambiguous and the proofs must be reducible to a succession of applications of inference rules,[e] without any use of empirical evidence and intuition.[f][177] Rigorous reasoning is not specific to mathematics, but, in mathematics, the standard of rigor is much higher than elsewhere. Despite mathematics' concision, rigorous proofs can require hundreds of pages to express, such as the 255-page Feit–Thompson theorem.[g] The emergence of computer-assisted proofs has allowed proof lengths to further expand.[h][178]  The result of this trend is a philosophy of the quasi-empiricist proof that can not be considered infallible, but has a probability attached to it.[6]

The concept of rigor in mathematics dates back to ancient Greece, where their society encouraged logical, deductive reasoning. However, this rigorous approach would tend to discourage exploration of new approaches, such as irrational numbers and concepts of infinity. The method of demonstrating rigorous proof was enhanced in the sixteenth century through the use of symbolic notation. In the 18th century, social transition led to mathematicians earning their keep through teaching, which led to more careful thinking about the underlying concepts of mathematics. This produced more rigorous approaches, while transitioning from geometric methods to algebraic and then arithmetic proofs.[6]

At the end of the 19th century, it appeared that the definitions of the basic concepts of mathematics were not accurate enough for avoiding paradoxes (non-Euclidean geometries and Weierstrass function) and contradictions (Russell's paradox). This was solved by the inclusion of axioms with the apodictic inference rules of mathematical theories; the re-introduction of axiomatic method pioneered by the ancient Greeks.[6] It results that ""rigor"" is no more a relevant concept in mathematics, as a proof is either correct or erroneous, and a ""rigorous proof"" is simply a pleonasm. Where a special concept of rigor comes into play is in the socialized aspects of a proof, wherein it may be demonstrably refuted by other mathematicians. After a proof has been accepted for many years or even decades, it can then be considered as reliable.[179]

Nevertheless, the concept of ""rigor"" may remain useful for teaching to beginners what is a mathematical proof.[180]

Mathematics has a remarkable ability to cross cultural boundaries and time periods. As a human activity, the practice of mathematics has a social side, which includes education, careers, recognition, popularization, and so on. In education, mathematics is a core part of the curriculum and forms an important element of the STEM academic disciplines. Prominent careers for professional mathematicians include math teacher or professor, statistician, actuary, financial analyst, economist, accountant, commodity trader, or computer consultant.[181]

Archaeological evidence shows that instruction in mathematics occurred as early as the second millennium BCE in ancient Babylonia.[182] Comparable evidence has been unearthed for scribal mathematics training in the ancient Near East and then for the Greco-Roman world starting around 300 BCE.[183] The oldest known mathematics textbook is the Rhind papyrus, dated from c. 1650 BCE in Egypt.[184] Due to a scarcity of books, mathematical teachings in ancient India were communicated using memorized oral tradition since the Vedic period (c. 1500 – c. 500 BCE).[185] In Imperial China during the Tang dynasty (618–907 CE), a mathematics curriculum was adopted for the civil service exam to join the state bureaucracy.[186]

Following the Dark Ages, mathematics education in Europe was provided by religious schools as part of the Quadrivium. Formal instruction in pedagogy began with Jesuit schools in the 16th and 17th century. Most mathematical curricula remained at a basic and practical level until the nineteenth century, when it began to flourish in France and Germany. The oldest journal addressing instruction in mathematics was L'Enseignement Mathématique, which began publication in 1899.[187] The Western advancements in science and technology led to the establishment of centralized education systems in many nation-states, with mathematics as a core component—initially for its military applications.[188] While the content of courses varies, in the present day nearly all countries teach mathematics to students for significant amounts of time.[189]

During school, mathematical capabilities and positive expectations have a strong association with career interest in the field. Extrinsic factors such as feedback motivation by teachers, parents, and peer groups can influence the level of interest in mathematics.[190] Some students studying math may develop an apprehension or fear about their performance in the subject. This is known as math anxiety or math phobia, and is considered the most prominent of the disorders impacting academic performance. Math anxiety can develop due to various factors such as parental and teacher attitudes, social stereotypes, and personal traits. Help to counteract the anxiety can come from changes in instructional approaches, by interactions with parents and teachers, and by tailored treatments for the individual.[191]

The validity of a mathematical theorem relies only on the rigor of its proof, which could theoretically be done automatically by a computer program. This does not mean that there is no place for creativity in a mathematical work. On the contrary, many important mathematical results (theorems) are solutions of problems that other mathematicians failed to solve, and the invention of a way for solving them may be a fundamental way of the solving process.[192][193] An extreme example is Apery's theorem: Roger Apery provided only the ideas for a proof, and the formal proof was given only several months later by three other mathematicians.[194]

Creativity and rigor are not the only psychological aspects of the activity of mathematicians. Some mathematicians can see their activity as a game, more specifically as solving puzzles.[195] This aspect of mathematical activity is emphasized in recreational mathematics.

Mathematicians can find an aesthetic value to mathematics. Like beauty, it is hard to define, it is commonly related to elegance, which involves qualities like simplicity, symmetry, completeness, and generality. G. H. Hardy in A Mathematician's Apology expressed the belief that the aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He also identified other criteria such as significance, unexpectedness, and inevitability, which contribute to mathematical aesthetics.[196] Paul Erdős expressed this sentiment more ironically by speaking of ""The Book"", a supposed divine collection of the most beautiful proofs. The 1998 book Proofs from THE BOOK, inspired by Erdős, is a collection of particularly succinct and revelatory mathematical arguments. Some examples of particularly elegant results included are Euclid's proof that there are infinitely many prime numbers and the fast Fourier transform for harmonic analysis.[197]

Some feel that to consider mathematics a science is to downplay its artistry and history in the seven traditional liberal arts.[198] One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematical results are created (as in art) or discovered (as in science).[131] The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.

Notes that sound well together to a Western ear are sounds whose fundamental frequencies of vibration are in simple ratios. For example, an octave doubles the frequency and a perfect fifth multiplies it by 





3
2




{\displaystyle {\frac {3}{2}}}

.[199][200]

Humans, as well as some other animals, find symmetric patterns to be more beautiful.[201] Mathematically, the symmetries of an object form a group known as the symmetry group.[202] For example, the group underlying mirror symmetry is the cyclic group of two elements, 




Z


/

2

Z



{\displaystyle \mathbb {Z} /2\mathbb {Z} }

. A Rorschach test is a figure invariant by this symmetry,[203] as are butterfly and animal bodies more generally (at least on the surface).[204] Waves on the sea surface possess translation symmetry: moving one's viewpoint by the distance between wave crests does not change one's view of the sea.[205] Fractals possess self-similarity.[206][207]

Popular mathematics is the act of presenting mathematics without technical terms.[208] Presenting mathematics may be hard since the general public suffers from mathematical anxiety and mathematical objects are highly abstract.[209] However, popular mathematics writing can overcome this by using applications or cultural links.[210] Despite this, mathematics is rarely the topic of popularization in printed or televised media.

The most prestigious award in mathematics is the Fields Medal,[211][212] established in 1936 and awarded every four years (except around World War II) to up to four individuals.[213][214] It is considered the mathematical equivalent of the Nobel Prize.[214]

Other prestigious mathematics awards include:[215]

A famous list of 23 open problems, called ""Hilbert's problems"", was compiled in 1900 by German mathematician David Hilbert.[223] This list has achieved great celebrity among mathematicians,[224] and at least thirteen of the problems (depending how some are interpreted) have been solved.[223]

A new list of seven important problems, titled the ""Millennium Prize Problems"", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a 1 million dollar reward.[225] To date, only one of these problems, the Poincaré conjecture, has been solved by the Russian mathematician Grigori Perelman.[226]
"

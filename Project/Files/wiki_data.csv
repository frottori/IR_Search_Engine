title,content
Science,"Science is a systematic discipline that builds and organises knowledge in the form of testable hypotheses and predictions about the universe.[1][2] Modern science is typically divided into two or three major branches:[3] the natural sciences (e.g., physics, chemistry, and biology), which study the physical world; and the behavioural sciences (e.g., economics, psychology, and sociology), which study individuals and societies.[4][5] The formal sciences (e.g., logic, mathematics, and theoretical computer science), which study formal systems governed by axioms and rules,[6][7] are sometimes described as being sciences as well; however, they are often regarded as a separate field because they rely on deductive reasoning instead of the scientific method or empirical evidence as their main methodology.[8][9] Applied sciences are disciplines that use scientific knowledge for practical purposes, such as engineering and medicine.[10][11][12] The history of science spans the majority of the historical record, with the earliest identifiable predecessors to modern science dating to the Bronze Age in Egypt and Mesopotamia (c. 3000–1200 BCE). Their contributions to mathematics, astronomy, and medicine entered and shaped the Greek natural philosophy of classical antiquity, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes, while further advancements, including the introduction of the Hindu–Arabic numeral system, were made during the Golden Age of India.[13]: 12 [14][15][16] Scientific research deteriorated in these regions after the fall of the Western Roman Empire during the Early Middle Ages (400–1000 CE), but in the Medieval renaissances (Carolingian Renaissance, Ottonian Renaissance and the Renaissance of the 12th century) scholarship flourished again. Some Greek manuscripts lost in Western Europe were preserved and expanded upon in the Middle East during the Islamic Golden Age,[17] along with the later efforts of Byzantine Greek scholars who brought Greek manuscripts from the dying Byzantine Empire to Western Europe at the start of the Renaissance. The recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th centuries revived natural philosophy,[18][19][20] which was later transformed by the Scientific Revolution that began in the 16th century[21] as new ideas and discoveries departed from previous Greek conceptions and traditions.[22][23] The scientific method soon played a greater role in knowledge creation and it was not until the 19th century that many of the institutional and professional features of science began to take shape,[24][25] along with the changing of ""natural philosophy"" to ""natural science"".[26] New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems.[27][28] Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions,[29] government agencies,[30] and companies.[31] The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritising the ethical and moral development of commercial products, armaments, health care, public infrastructure, and environmental protection. The word science has been used in Middle English since the 14th century in the sense of ""the state of knowing"". The word was borrowed from the Anglo-Norman language as the suffix -cience, which was borrowed from the Latin word scientia, meaning ""knowledge, awareness, understanding"". It is a noun derivative of the Latin sciens meaning ""knowing"", and undisputedly derived from the Latin sciō, the present participle scīre, meaning ""to know"".[32] There are many hypotheses for science's ultimate word origin. According to Michiel de Vaan, Dutch linguist and Indo-Europeanist, sciō may have its origin in the Proto-Italic language as *skije- or *skijo- meaning ""to know"", which may originate from Proto-Indo-European language as *skh1-ie, *skh1-io, meaning ""to incise"". The Lexikon der indogermanischen Verben proposed sciō is a back-formation of nescīre, meaning ""to not know, be unfamiliar with"", which may derive from Proto-Indo-European *sekH- in Latin secāre, or *skh2-, from *sḱʰeh2(i)- meaning ""to cut"".[33] In the past, science was a synonym for ""knowledge"" or ""study"", in keeping with its Latin origin. A person who conducted scientific research was called a ""natural philosopher"" or ""man of science"".[34] In 1834, William Whewell introduced the term scientist in a review of Mary Somerville's book On the Connexion of the Physical Sciences,[35] crediting it to ""some ingenious gentleman"" (possibly himself).[36] Science has no single origin. Rather, scientific thinking emerged gradually over the course of tens of thousands of years,[37][38] taking different forms around the world, and few details are known about the very earliest developments. Women likely played a central role in prehistoric science,[39] as did religious rituals.[40] Some scholars use the term ""protoscience"" to label activities in the past that resemble modern science in some but not all features;[41][42][43] however, this label has also been criticised as denigrating,[44] or too suggestive of presentism, thinking about those activities only in relation to modern categories.[45] Direct evidence for scientific processes becomes clearer with the advent of writing systems in the Bronze Age civilisations of Ancient Egypt and Mesopotamia (c. 3000–1200 BCE), creating the earliest written records in the history of science.[13]: 12–15 [14] Although the words and concepts of ""science"" and ""nature"" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine.[46][13]: 12 From the 3rd millennium BCE, the ancient Egyptians developed a non-positional decimal numbering system,[47] solved practical problems using geometry,[48] and developed a calendar.[49] Their healing therapies involved drug treatments and the supernatural, such as prayers, incantations, and rituals.[13]: 9 The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing.[50] They studied animal physiology, anatomy, behaviour, and astrology for divinatory purposes.[51] The Mesopotamians had an intense interest in medicine and the earliest medical prescriptions appeared in Sumerian during the Third Dynasty of Ur.[50][52] They seem to have studied scientific subjects which had practical or religious applications and had little interest in satisfying curiosity.[50] In classical antiquity, there is no real ancient analogue of a modern scientist. Instead, well-educated, usually upper-class, and almost"
Technology,"Technology is the application of conceptual knowledge to achieve practical goals, especially in a reproducible way.[1] The word technology can also mean the products resulting from such efforts,[2][3] including both tangible tools such as utensils or machines, and intangible ones such as software. Technology plays a critical role in science, engineering, and everyday life. Technological advancements have led to significant changes in society. The earliest known technology is the stone tool, used during prehistory, followed by the control of fire—which in turn contributed to the growth of the human brain and the development of language during the Ice Age, according to the cooking hypothesis. The invention of the wheel in the Bronze Age allowed greater travel and the creation of more complex machines. More recent technological inventions, including the printing press, telephone, and the Internet, have lowered barriers to communication and ushered in the knowledge economy. While technology contributes to economic development and improves human prosperity, it can also have negative impacts like pollution and resource depletion, and can cause social harms like technological unemployment resulting from automation. As a result, philosophical and political debates about the role and use of technology, the ethics of technology, and ways to mitigate its downsides are ongoing. Technology is a term dating back to the early 17th century that meant 'systematic treatment' (from Greek Τεχνολογία, from the Greek: τέχνη, romanized: tékhnē, lit. 'craft, art' and -λογία (-logíā), 'study, knowledge').[4][5] It is predated in use by the Ancient Greek word τέχνη (tékhnē), used to mean 'knowledge of how to make things', which encompassed activities like architecture.[6] Starting in the 19th century, continental Europeans started using the terms Technik (German) or technique (French) to refer to a 'way of doing', which included all technical arts, such as dancing, navigation, or printing, whether or not they required tools or instruments.[7] At the time, Technologie (German and French) referred either to the academic discipline studying the ""methods of arts and crafts"", or to the political discipline ""intended to legislate on the functions of the arts and crafts.""[8] The distinction between Technik and Technologie is absent in English, and so both were translated as technology. The term was previously uncommon in English and mostly referred to the academic discipline, as in the Massachusetts Institute of Technology.[9] In the 20th century, as a result of scientific progress and the Second Industrial Revolution, technology stopped being considered a distinct academic discipline and took on the meaning: the systemic use of knowledge to practical ends.[10] Tools were initially developed by hominids through observation and trial and error.[11] Around 2 Mya (million years ago), they learned to make the first stone tools by hammering flakes off a pebble, forming a sharp hand axe.[12] This practice was refined 75 kya (thousand years ago) into pressure flaking, enabling much finer work.[13] The discovery of fire was described by Charles Darwin as ""possibly the greatest ever made by man"".[14] Archaeological, dietary, and social evidence point to ""continuous [human] fire-use"" at least 1.5 Mya.[15] Fire, fueled with wood and charcoal, allowed early humans to cook their food to increase its digestibility, improving its nutrient value and broadening the number of foods that could be eaten.[16] The cooking hypothesis proposes that the ability to cook promoted an increase in hominid brain size, though some researchers find the evidence inconclusive.[17] Archaeological evidence of hearths was dated to 790 kya; researchers believe this is likely to have intensified human socialization and may have contributed to the emergence of language.[18][19] Other technological advances made during the Paleolithic era include clothing and shelter.[20] No consensus exists on the approximate time of adoption of either technology, but archaeologists have found archaeological evidence of clothing 90-120 kya[21] and shelter 450 kya.[20] As the Paleolithic era progressed, dwellings became more sophisticated and more elaborate; as early as 380 kya, humans were constructing temporary wood huts.[22][23] Clothing, adapted from the fur and hides of hunted animals, helped humanity expand into colder regions; humans began to migrate out of Africa around 200 kya, initially moving to Eurasia.[24][25][26] The Neolithic Revolution (or First Agricultural Revolution) brought about an acceleration of technological innovation, and a consequent increase in social complexity.[27] The invention of the polished stone axe was a major advance that allowed large-scale forest clearance and farming.[28] This use of polished stone axes increased greatly in the Neolithic but was originally used in the preceding Mesolithic in some areas such as Ireland.[29] Agriculture fed larger populations, and the transition to sedentism allowed for the simultaneous raising of more children, as infants no longer needed to be carried around by nomads. Additionally, children could contribute labor to the raising of crops more readily than they could participate in hunter-gatherer activities.[30][31] With this increase in population and availability of labor came an increase in labor specialization.[32] What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations, such as Sumer, is not specifically known; however, the emergence of increasingly hierarchical social structures and specialized labor, of trade and war among adjacent cultures, and the need for collective action to overcome environmental challenges such as irrigation, are all thought to have played a role.[33] The invention of writing led to the spread of cultural knowledge and became the basis for history, libraries, schools, and scientific research.[34] Continuing improvements led to the furnace and bellows and provided, for the first time, the ability to smelt and forge gold, copper, silver, and lead – native metals found in relatively pure form in nature.[35] The advantages of copper tools over stone, bone and wooden tools were quickly apparent to early humans, and native copper was probably used from near the beginning of Neolithic times (about 10 kya).[36] Native copper does not naturally occur in large amounts, but copper ores are quite common and some of them produce metal easily when burned in wood or charcoal fires. Eventually, the working of metals led to the discovery of alloys such as bronze and brass (about 4,000 BCE)."
Engineering,"Engineering is the practice of using natural science, mathematics, and the engineering design process[1] to solve technical problems, increase efficiency and productivity, and improve systems. Modern engineering comprises many subfields which include designing and improving infrastructure, machinery, vehicles, electronics, materials, and energy systems.[2] The discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied mathematics, applied science, and types of application. See glossary of engineering. The term engineering is derived from the Latin ingenium, meaning ""cleverness"".[3] The American Engineers' Council for Professional Development (ECPD, the predecessor of ABET)[4] has defined ""engineering"" as: The creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation and safety to life and property.[5][6] Engineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley, etc. The term engineering is derived from the word engineer, which itself dates back to the 14th century when an engine'er (literally, one who builds or operates a siege engine) referred to ""a constructor of military engines"".[7] In this context, now obsolete, an ""engine"" referred to a military machine, i.e., a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, e.g., the U.S. Army Corps of Engineers. The word ""engine"" itself is of even older origin, ultimately deriving from the Latin ingenium (c. 1250), meaning ""innate quality, especially mental power, hence a clever invention.""[8] Later, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering[6] entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering. The pyramids in ancient Egypt, ziggurats of Mesopotamia, the Acropolis and Parthenon in Greece, the Roman aqueducts, Via Appia and Colosseum, Teotihuacán, and the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon and the Pharos of Alexandria, were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World. The six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times.[9] The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC.[10] The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale,[11] and to move large objects in ancient Egyptian technology.[12] The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia c. 3000 BC,[11] and then in ancient Egyptian technology c. 2000 BC.[13] The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC,[14] and ancient Egypt during the Twelfth Dynasty (1991–1802 BC).[15] The screw, the last of the simple machines to be invented,[16] first appeared in Mesopotamia during the Neo-Assyrian period (911–609) BC.[14] The Egyptian pyramids were built using three of the six simple machines, the inclined plane, the wedge, and the lever, to create structures like the Great Pyramid of Giza.[17] The earliest civil engineer known by name is Imhotep.[6] As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630–2611 BC.[18] The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC.[19] Kush developed the Sakia during the 4th century BC, which relied on animal power instead of human energy.[20] Hafirs were developed as a type of reservoir in Kush to store and contain water as well as boost irrigation.[21] Sappers were employed to build causeways during military campaigns.[22] Kushite ancestors built speos during the Bronze Age between 3700 and 3250 BC.[23] Bloomeries and blast furnaces were also created during the 7th centuries BC in Kush.[24][25][26][27] Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, an early known mechanical analog computer,[28][29] and the mechanical inventions of Archimedes, are examples of Greek mechanical engineering. Some of Archimedes' inventions, as well as the Antikythera mechanism, required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are widely used in fields such as robotics and automotive engineering.[30] Ancient Chinese, Greek, Roman and Hunnic armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century BC,[31] the trireme, the ballista and the catapult. In the Middle Ages, the trebuchet was developed. The earliest practical wind-powered machines, the windmill and wind pump, first appeared in the Muslim world during the Islamic Golden Age, in what are now Iran, Afghanistan, and Pakistan, by the 9th century AD.[32][33][34][35] The earliest practical steam-powered machine was a steam jack driven by a steam turbine, described in 1551 by Taqi al-Din Muhammad ibn Ma'ruf in Ottoman Egypt.[36][37] The cotton gin was invented in India by the 6th century AD,[38] and the spinning wheel was invented in the Islamic world by the early 11th century,[39] both of which were fundamental to the growth of the cotton industry. The spinning wheel was also a precursor to the spinning jenny, which was a key development during the early Industrial Revolution in the 18th century.[40] The earliest programmable machines were developed in the Muslim world."
Computer,"A computer is a machine that can be programmed to automatically carry out sequences of arithmetic or logical operations (computation). Modern digital electronic computers can perform generic sets of operations known as programs. These programs enable computers to perform a wide range of tasks. The term computer system may refer to a nominally complete computer that includes the hardware, operating system, software, and peripheral equipment needed and used for full operation; or to a group of computers that are linked and function together, such as a computer network or computer cluster. A broad range of industrial and consumer products use computers as control systems, including simple special-purpose devices like microwave ovens and remote controls, and factory devices like industrial robots. Computers are at the core of general-purpose devices such as personal computers and mobile devices such as smartphones. Computers power the Internet, which links billions of computers and users. Early computers were meant to be used only for calculations. Simple manual instruments like the abacus have aided people in doing calculations since ancient times. Early in the Industrial Revolution, some mechanical devices were built to automate long, tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II, both electromechanical and using thermionic valves. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power, and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (Moore's law noted that counts doubled every two years), leading to the Digital Revolution during the late 20th and early 21st centuries. Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, together with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joysticks, etc.), output devices (monitors, printers, etc.), and input/output devices that perform both functions (e.g. touchscreens). Peripheral devices allow information to be retrieved from an external source, and they enable the results of operations to be saved and retrieved. It was not until the mid-20th century that the word acquired its modern definition; according to the Oxford English Dictionary, the first known use of the word computer was in a different sense, in a 1613 book called The Yong Mans Gleanings by the English writer Richard Brathwait: ""I haue [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number."" This usage of the term referred to a human computer, a person who carried out calculations or computations. The word continued to have the same meaning until the middle of the 20th century. During the latter part of this period, women were often hired as computers because they could be paid less than their male counterparts.[1] By 1943, most human computers were women.[2] The Online Etymology Dictionary gives the first attested use of computer in the 1640s, meaning 'one who calculates'; this is an ""agent noun from compute (v.)"". The Online Etymology Dictionary states that the use of the term to mean ""'calculating machine' (of any type) is from 1897."" The Online Etymology Dictionary indicates that the ""modern use"" of the term, to mean 'programmable digital electronic computer' dates from ""1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine"".[3] The name has remained, although modern computers are capable of many higher-level functions. Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was most likely a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, likely livestock or grains, sealed in hollow unbaked clay containers.[a][4] The use of counting rods is one example. The abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BCE. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.[5] The Antikythera mechanism is believed to be the earliest known mechanical analog computer, according to Derek J. de Solla Price.[6] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to approximately c. 100 BCE. Devices of comparable complexity to the Antikythera mechanism would not reappear until the fourteenth century.[7] Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century.[8] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BCE and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[9][10] and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[11] Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe,[12] an early fixed-wired knowledge processing machine[13] with a gear train and gear-wheels,[14] c. 1000 AD. The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the"
Mathematics,"Mathematics is a field of study that discovers and organizes methods, theories and theorems that are developed and proved for the needs of empirical sciences and mathematics itself. There are many areas of mathematics, which include number theory (the study of numbers), algebra (the study of formulas and related structures), geometry (the study of shapes and spaces that contain them), analysis (the study of continuous changes), and set theory (presently used as a foundation for all mathematics). Mathematics involves the description and manipulation of abstract objects that consist of either abstractions from nature or—in modern mathematics—purely abstract entities that are stipulated to have certain properties, called axioms. Mathematics uses pure reason to prove properties of objects, a proof consisting of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered true starting points of the theory under consideration.[1] Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science, and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent of any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics) but often later find practical applications.[2][3] Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements.[4] Since its beginning, mathematics was primarily divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra[a] and infinitesimal calculus were introduced as new fields. Since then, the interaction between mathematical innovations and scientific discoveries has led to a correlated increase in the development of both.[5] At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method,[6] which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than sixty first-level areas of mathematics. Before the Renaissance, mathematics was divided into two main areas: arithmetic, regarding the manipulation of numbers, and geometry, regarding the study of shapes.[7] Some types of pseudoscience, such as numerology and astrology, were not then clearly distinguished from mathematics.[8] During the Renaissance, two more areas appeared. Mathematical notation led to algebra which, roughly speaking, consists of the study and the manipulation of formulas. Calculus, consisting of the two subfields differential calculus and integral calculus, is the study of continuous functions, which model the typically nonlinear relationships between varying quantities, as represented by variables. This division into four main areas—arithmetic, geometry, algebra, and calculus[9]—endured until the end of the 19th century. Areas such as celestial mechanics and solid mechanics were then studied by mathematicians, but now are considered as belonging to physics.[10] The subject of combinatorics has been studied for much of recorded history, yet did not become a separate branch of mathematics until the seventeenth century.[11] At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics.[12][6] The 2020 Mathematics Subject Classification contains no less than sixty-three first-level areas.[13] Some of these areas correspond to the older division, as is true regarding number theory (the modern name for higher arithmetic) and geometry. Several other first-level areas have ""geometry"" in their names or are otherwise commonly considered part of geometry. Algebra and calculus do not appear as first-level areas but are respectively split into several first-level areas. Other first-level areas emerged during the 20th century or had not previously been considered as mathematics, such as mathematical logic and foundations.[14] Number theory began with the manipulation of numbers, that is, natural numbers ( N ) , {\displaystyle (\mathbb {N} ),} and later expanded to integers ( Z ) {\displaystyle (\mathbb {Z} )} and rational numbers ( Q ) . {\displaystyle (\mathbb {Q} ).} Number theory was once called arithmetic, but nowadays this term is mostly used for numerical calculations.[15] Number theory dates back to ancient Babylon and probably China. Two prominent early number theorists were Euclid of ancient Greece and Diophantus of Alexandria.[16] The modern study of number theory in its abstract form is largely attributed to Pierre de Fermat and Leonhard Euler. The field came to full fruition with the contributions of Adrien-Marie Legendre and Carl Friedrich Gauss.[17] Many easily stated number problems have solutions that require sophisticated methods, often from across mathematics. A prominent example is Fermat's Last Theorem. This conjecture was stated in 1637 by Pierre de Fermat, but it was proved only in 1994 by Andrew Wiles, who used tools including scheme theory from algebraic geometry, category theory, and homological algebra.[18] Another example is Goldbach's conjecture, which asserts that every even integer greater than 2 is the sum of two prime numbers. Stated in 1742 by Christian Goldbach, it remains unproven despite considerable effort.[19] Number theory includes several subareas, including analytic number theory, algebraic number theory, geometry of numbers (method oriented), diophantine equations, and transcendence theory (problem oriented).[14] Geometry is one of the oldest branches of mathematics. It started with empirical recipes concerning shapes, such as lines, angles and circles, which were developed mainly for the needs of surveying and architecture, but has since blossomed out into many other subfields.[20] A fundamental innovation was the ancient Greeks' introduction of the concept of proofs, which require that every assertion must be proved. For example, it is not sufficient to verify by measurement that, say, two lengths are equal; their equality must be proven via reasoning from previously accepted results (theorems) and a few basic statements. The basic statements are not subject to proof because they are self-evident (postulates), or are part of the definition of the subject of study (axioms). This principle, foundational for all mathematics, was first elaborated"
Robotics,"Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.[1] Within mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering. The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes. Robotics usually combines three aspects of design work to create robot systems: As many robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed ""assembly robots"". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a ""welding robot"" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as ""heavy-duty robots"".[4] Current and potential applications include: At present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[16] Potential power sources could be: Actuators are the ""muscles"" of a robot, the parts which convert stored energy into movement.[17] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air. The vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational. Various types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car. Series elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[18] Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[19] and walking humanoid robots.[20][21] The controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments.[22] Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions.[23] One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA.[24] This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains. Pneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[25][26][27] Muscle wire, also known as shape memory alloy, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[28][29] EAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[30] and to enable new robots to float,[31] fly, swim or walk.[32] Recent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[33] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of"
Machine learning,"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.[2] ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business problems is known as predictive analytics. Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[6][7] From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning. The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.[10][11] Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[12] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[13] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[12] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[12] By the early 1960s, an experimental ""learning machine"" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively ""trained"" by a human operator/teacher to recognize patterns and equipped with a ""goof"" button to cause it to reevaluate incorrect decisions.[14] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[15] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[16] In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[17] Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: ""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.""[18] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper ""Computing Machinery and Intelligence"", in which the question ""Can machines think?"" is replaced with the question ""Can machines do what we (as thinking entities) can do?"".[19] Modern-day machine learning has two objectives. One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.[20] As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ""neural networks""; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[22] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[23]: 488 However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[23]: 488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[24] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[23]: 708–710, 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as ""connectionism"", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[23]: 25 Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[24] There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for ""general intelligence"".[25][26][27] An alternative view can show compression algorithms implicitly map"
History,"History (derived from Ancient Greek ἱστορία (historía) 'inquiry; knowledge acquired by investigation')[1] is the systematic study and documentation of the human past.[2][3] History is an academic discipline which uses a narrative to describe, examine, question, and analyse past events, and investigate their patterns of cause and effect.[4][5] Historians debate which narrative best explains an event, as well as the significance of different causes and effects. Historians debate the nature of history as an end in itself, and its usefulness in giving perspective on the problems of the present.[4][6][7][8] The period of events before the invention of writing systems is considered prehistory.[9] ""History"" is an umbrella term comprising past events as well as the memory, discovery, collection, organization, presentation, and interpretation of these events. Historians seek knowledge of the past using historical sources such as written documents, oral accounts or traditional oral histories, art and material artefacts, and ecological markers.[10] Stories common to a particular culture, but not supported by external sources (such as the tales surrounding King Arthur), are usually classified as cultural heritage or legends.[11][12] History differs from myth in that it is supported by verifiable evidence. However, ancient cultural influences have helped create variant interpretations of the nature of history, which have evolved over the centuries and continue to change today. The modern study of history is wide-ranging, and includes the study of specific regions and certain topical or thematic elements of historical investigation. History is taught as a part of primary and secondary education, and the academic study of history is a major discipline in universities. Herodotus, a 5th-century BCE Greek historian, is often considered the ""father of history"", as one of the first historians in the Western tradition,[13] though he has been criticized as the ""father of lies"".[14][15] Along with his contemporary Thucydides, he helped form the foundations for the modern study of past events and societies.[16] Their works continue to be read today, and the gap between the culture-focused Herodotus and the military-focused Thucydides remains a point of contention or approach in modern historical writing. In East Asia a state chronicle, the Spring and Autumn Annals, was reputed to date from as early as 722 BCE, though only 2nd-century BCE texts have survived. The title ""father of history"" has also been attributed, in their respective societies, to Sima Qian, Ibn Khaldun, and Kenneth Dike.[17][18][19] As an academic discipline, history is the study of the past.[20] It conceptualizes and describes what happened by collecting and analysing evidence to construct narratives. These narratives cover not only how events unfolded but also why they happened and in which contexts, providing an explanation of relevant background conditions and causal mechanisms. History further examines the meaning of historical events and the underlying human motives driving them.[21] In a slightly different sense, history refers to the past events themselves. In this sense, history is what happened rather than the academic field studying what happened. When used as a countable noun, a history is a representation of the past in the form of a history text. History texts are cultural products involving active interpretation and reconstruction. The narratives presented in them can change as historians discover new evidence or reinterpret already-known sources. The nature of the past itself, by contrast, is static and unchangeable.[22] Some historians focus on the interpretative and explanatory aspects to distinguish histories from chronicles, arguing that chronicles only catalogue events in chronological order, whereas histories aim at a comprehensive understanding of their causes, contexts, and consequences.[23][a] Traditionally, history was primarily concerned with written documents. It focused on recorded history since the invention of writing, leaving prehistory[b] to other fields, such as archaeology.[26] Today, history has a broader scope that includes prehistory, starting with the earliest human origins several million years ago.[27][c] It is controversial whether history is a social science or forms part of the humanities. Like social scientists, historians formulate hypotheses, gather objective evidence, and present arguments based on this evidence. At the same time, history aligns closely with the humanities because of its reliance on subjective aspects associated with interpretation, storytelling, human experience, and cultural heritage.[29] Some historians strongly support one or the other classification while others characterize history as a hybrid discipline that does not belong to one category at the exclusion of the other.[30] History contrasts with pseudohistory, which deviates from historiographical standards by relying on disputed historical evidence, selectively ignoring genuine evidence, or using other means to distort the historical record. Often motivated by specific ideological agendas, pseudohistorians mimic historical methodology to promote misleading narratives that lack rigorous analysis and scholarly consensus.[31] Various suggestions about the purpose or value of history have been made. Some historians propose that its primary function is the pure discovery of the truth about the past. This view emphasizes that the disinterested pursuit of truth is an end in itself, while external purposes, associated with ideology or politics, threaten to undermine the accuracy of historical research by distorting the past. In this role, history also challenges traditional myths lacking factual support.[32] A different perspective suggests that the main value of history lies in the lessons it teaches for the present. This view is based on the idea that an understanding of the past can guide decision-making, for example, to avoid repeating previous mistakes.[33] A related perspective focuses on a general understanding of the human condition, making people aware of the diversity of human behaviour across different contexts—similar to what one can learn by visiting foreign countries.[34] History can also foster social cohesion by providing people with a collective identity through a shared past, helping to preserve and cultivate cultural heritage and values across generations.[35] History is sometimes used for political or ideological purposes, for instance, to justify the status quo by making certain traditions appear respectable or to promote change by highlighting past injustices.[36] Pushed to extreme forms, this can result in pseudohistory or historical denialism when evidence is intentionally ignored or misinterpreted to construct a misleading narrative serving external interests.[37] The word history comes from the Ancient"
Art,"Art describes a diverse range of cultural activity centered around works utilizing creative or imaginative talents, which are expected to evoke a worthwhile experience,[1] generally through an expression of emotional power, conceptual ideas, technical proficiency, and/or beauty.[2][3][4] There is no generally agreed definition of what constitutes art,[5][6][7] and its interpretation has varied greatly throughout history and across cultures. In the Western tradition, the three classical branches of visual art are painting, sculpture, and architecture.[8] Theatre, dance, and other performing arts, as well as literature, music, film and other media such as interactive media, are included in a broader definition of ""the arts"".[2][9] Until the 17th century, art referred to any skill or mastery and was not differentiated from crafts or sciences. In modern usage after the 17th century, where aesthetic considerations are paramount, the fine arts are separated and distinguished from acquired skills in general, such as the decorative or applied arts. The nature of art and related concepts, such as creativity and interpretation, are explored in a branch of philosophy known as aesthetics.[10] The resulting artworks are studied in the professional fields of art criticism and the history of art. In the perspective of the history of art,[11] artistic works have existed for almost as long as humankind: from early prehistoric art to contemporary art; however, some theorists think that the typical concept of ""artistic works"" does not fit well outside modern Western societies.[12] One early sense of the definition of art is closely related to the older Latin meaning, which roughly translates to ""skill"" or ""craft"", as associated with words such as ""artisan"". English words derived from this meaning include artifact, artificial, artifice, medical arts, and military arts. However, there are many other colloquial uses of the word, all with some relation to its etymology. Over time, philosophers like Plato, Aristotle, Socrates and Immanuel Kant, among others, questioned the meaning of art.[13] Several dialogues in Plato tackle questions about art, while Socrates says that poetry is inspired by the muses and is not rational. He speaks approvingly of this, and other forms of divine madness (drunkenness, eroticism, and dreaming) in the Phaedrus (265a–c), and yet in the Republic wants to outlaw Homer's great poetic art, and laughter as well. In Ion, Socrates gives no hint of the disapproval of Homer that he expresses in the Republic. The dialogue Ion suggests that Homer's Iliad functioned in the ancient Greek world as the Bible does today in the modern Christian world: as divinely inspired literary art that can provide moral guidance, if only it can be properly interpreted.[14] With regards to the literary art and the musical arts, Aristotle considered epic poetry, tragedy, comedy, Dithyrambic poetry and music to be mimetic or imitative art, each varying in imitation by medium, object, and manner.[15] For example, music imitates with the media of rhythm and harmony, whereas dance imitates with rhythm alone, and poetry with language. The forms also differ in their object of imitation. Comedy, for instance, is a dramatic imitation of men worse than average; whereas tragedy imitates men slightly better than average. Lastly, the forms differ in their manner of imitation—through narrative or character, through change or no change, and through drama or no drama.[16] Aristotle believed that imitation is natural to mankind and constitutes one of mankind's advantages over animals.[17] The more recent and specific sense of the word art as an abbreviation for creative art or fine art emerged in the early 17th century.[18] Fine art refers to a skill used to express the artist's creativity, or to engage the audience's aesthetic sensibilities, or to draw the audience towards consideration of more refined or finer works of art. Within this latter sense, the word art may refer to several things: (i) a study of a creative skill, (ii) a process of using the creative skill, (iii) a product of the creative skill, or (iv) the audience's experience with the creative skill. The creative arts (art as discipline) are a collection of disciplines which produce artworks (art as objects) that are compelled by a personal drive (art as activity) and convey a message, mood, or symbolism for the perceiver to interpret (art as experience). Art is something that stimulates an individual's thoughts, emotions, beliefs, or ideas through the senses. Works of art can be explicitly made for this purpose or interpreted on the basis of images or objects. For some scholars, such as Kant, the sciences and the arts could be distinguished by taking science as representing the domain of knowledge and the arts as representing the domain of the freedom of artistic expression.[19] Often, if the skill is being used in a common or practical way, people will consider it a craft instead of art. Likewise, if the skill is being used in a commercial or industrial way, it may be considered commercial art instead of fine art. On the other hand, crafts and design are sometimes considered applied art. Some art followers have argued that the difference between fine art and applied art has more to do with value judgments made about the art than any clear definitional difference.[20] However, even fine art often has goals beyond pure creativity and self-expression. The purpose of works of art may be to communicate ideas, such as in politically, spiritually, or philosophically motivated art; to create a sense of beauty (see aesthetics); to explore the nature of perception; for pleasure; or to generate strong emotions. The purpose may also be seemingly nonexistent. The nature of art has been described by philosopher Richard Wollheim as ""one of the most elusive of the traditional problems of human culture"".[21] Art has been defined as a vehicle for the expression or communication of emotions and ideas, a means for exploring and appreciating formal elements for their own sake, and as mimesis or representation. Art as mimesis has deep roots in the philosophy of Aristotle.[22] Leo Tolstoy identified art as a use of indirect means to communicate from one person to another.[22] Benedetto Croce"
Philosophy,"Philosophy ('love of wisdom' in Ancient Greek) is a systematic study of general and fundamental questions concerning topics like existence, reason, knowledge, value, mind, and language. It is a rational and critical inquiry that reflects on its methods and assumptions. Historically, many of the individual sciences, such as physics and psychology, formed part of philosophy. However, they are considered separate academic disciplines in the modern sense of the term. Influential traditions in the history of philosophy include Western, Arabic–Persian, Indian, and Chinese philosophy. Western philosophy originated in Ancient Greece and covers a wide area of philosophical subfields. A central topic in Arabic–Persian philosophy is the relation between reason and revelation. Indian philosophy combines the spiritual problem of how to reach enlightenment with the exploration of the nature of reality and the ways of arriving at knowledge. Chinese philosophy focuses principally on practical issues about right social conduct, government, and self-cultivation. Major branches of philosophy are epistemology, ethics, logic, and metaphysics. Epistemology studies what knowledge is and how to acquire it. Ethics investigates moral principles and what constitutes right conduct. Logic is the study of correct reasoning and explores how good arguments can be distinguished from bad ones. Metaphysics examines the most general features of reality, existence, objects, and properties. Other subfields are aesthetics, philosophy of language, philosophy of mind, philosophy of religion, philosophy of science, philosophy of mathematics, philosophy of history, and political philosophy. Within each branch, there are competing schools of philosophy that promote different principles, theories, or methods. Philosophers use a great variety of methods to arrive at philosophical knowledge. They include conceptual analysis, reliance on common sense and intuitions, use of thought experiments, analysis of ordinary language, description of experience, and critical questioning. Philosophy is related to many other fields, including the sciences, mathematics, business, law, and journalism. It provides an interdisciplinary perspective and studies the scope and fundamental concepts of these fields. It also investigates their methods and ethical implications. The word philosophy comes from the Ancient Greek words φίλος (philos) 'love' and σοφία (sophia) 'wisdom'.[2][a] Some sources say that the term was coined by the pre-Socratic philosopher Pythagoras, but this is not certain.[4] The word entered the English language primarily from Old French and Anglo-Norman starting around 1175 CE. The French philosophie is itself a borrowing from the Latin philosophia. The term philosophy acquired the meanings of ""advanced study of the speculative subjects (logic, ethics, physics, and metaphysics)"", ""deep wisdom consisting of love of truth and virtuous living"", ""profound learning as transmitted by the ancient writers"", and ""the study of the fundamental nature of knowledge, reality, and existence, and the basic limits of human understanding"".[5] Before the modern age, the term philosophy was used in a wide sense. It included most forms of rational inquiry, such as the individual sciences, as its subdisciplines.[6] For instance, natural philosophy was a major branch of philosophy.[7] This branch of philosophy encompassed a wide range of fields, including disciplines like physics, chemistry, and biology.[8] An example of this usage is the 1687 book Philosophiæ Naturalis Principia Mathematica by Isaac Newton. This book referred to natural philosophy in its title, but it is today considered a book of physics.[9] The meaning of philosophy changed toward the end of the modern period when it acquired the more narrow meaning common today. In this new sense, the term is mainly associated with philosophical disciplines like metaphysics, epistemology, and ethics. Among other topics, it covers the rational study of reality, knowledge, and values. It is distinguished from other disciplines of rational inquiry such as the empirical sciences and mathematics.[10] The practice of philosophy is characterized by several general features: it is a form of rational inquiry, it aims to be systematic, and it tends to critically reflect on its own methods and presuppositions.[11] It requires attentively thinking long and carefully about the provocative, vexing, and enduring problems central to the human condition.[12] The philosophical pursuit of wisdom involves asking general and fundamental questions. It often does not result in straightforward answers but may help a person to better understand the topic, examine their life, dispel confusion, and overcome prejudices and self-deceptive ideas associated with common sense.[13] For example, Socrates stated that ""the unexamined life is not worth living"" to highlight the role of philosophical inquiry in understanding one's own existence.[14][15] And according to Bertrand Russell, ""the man who has no tincture of philosophy goes through life imprisoned in the prejudices derived from common sense, from the habitual beliefs of his age or his nation, and from convictions which have grown up in his mind without the cooperation or consent of his deliberate reason.""[16] Attempts to provide more precise definitions of philosophy are controversial[17] and are studied in metaphilosophy.[18] Some approaches argue that there is a set of essential features shared by all parts of philosophy. Others see only weaker family resemblances or contend that it is merely an empty blanket term.[19] Precise definitions are often only accepted by theorists belonging to a certain philosophical movement and are revisionistic according to Søren Overgaard et al. in that many presumed parts of philosophy would not deserve the title ""philosophy"" if they were true.[20] Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the attempt to answer the big questions.[21] Such an approach is pursued by Immanuel Kant, who holds that the task of philosophy is united by four questions: ""What can I know?""; ""What should I do?""; ""What may I hope?""; and ""What is the human being?""[22] Both approaches have the problem that they are usually either too wide, by including non-philosophical disciplines, or too narrow, by excluding some philosophical sub-disciplines.[23] Many definitions of philosophy emphasize its intimate relation to science.[24] In this sense, philosophy is sometimes understood as a proper science in its own right. According to some naturalistic philosophers, such as W. V. O. Quine, philosophy is an"
Literature,"Literature is any collection of written work, but it is also used more narrowly for writings specifically considered to be an art form, especially novels, plays, and poems.[1] It includes both print and digital writing.[2] In recent centuries, the definition has expanded to include oral literature, much of which has been transcribed.[3][4] Literature is a method of recording, preserving, and transmitting knowledge and entertainment. It can also have a social, psychological, spiritual, or political role. Literary criticism is one of the oldest academic disciplines, and is concerned with the literary merit or intellectual significance of specific texts. The study of books and other texts as artifacts or traditions is instead encompassed by textual criticism or the history of the book. ""Literature"", as an art form, is sometimes used synonymously with literary fiction, fiction written with the goal of artistic merit,[5][6] but can also include works in various non-fiction genres, such as biography, diaries, memoirs, letters, and essays. Within this broader definition, literature includes non-fictional books, articles, or other written information on a particular subject.[7][8] Developments in print technology have allowed an ever-growing distribution and proliferation of written works, while the digital era had blurred the lines between online electronic literature and other forms of modern media. Definitions of literature have varied over time.[9] In Western Europe, prior to the 18th century, literature denoted all books and writing. It can be seen as returning to older, more inclusive notions, so that cultural studies, for instance, include, in addition to canonical works, popular and minority genres. The word is also used in reference to non-written works: to ""oral literature"" and ""the literature of preliterate culture"".[citation needed] Etymologically, the term derives from Latin literatura/litteratura, ""learning, writing, grammar,"" originally ""writing formed with letters,"" from litera/littera, ""letter.""[10] In spite of this, the term has also been applied to spoken or sung texts.[11][12] Literature is often referred to synecdochically as ""writing,"" especially creative writing, and poetically as ""the craft of writing"" (or simply ""the craft""). Syd Field described his discipline, screenwriting, as ""a craft that occasionally rises to the level of art.""[13] A value judgment definition of literature considers it as consisting solely of high quality writing that forms part of the belles-lettres (""fine writing"") tradition.[14] An example of this is in the 1910–1911 Encyclopædia Britannica, which classified literature as ""the best expression of the best thought reduced to writing"".[15] The use of the term ""literature"" here poses some issues due to its origins in the Latin littera, ""letter,"" essentially writing. Alternatives such as ""oral forms"" and ""oral genres"" have been suggested, but the word literature is widely used.[4] Australian Aboriginal culture has thrived on oral traditions and oral histories passed down through tens of thousands of years. In a study published in February 2020, new evidence showed that both Budj Bim and Tower Hill volcanoes erupted between 34,000 and 40,000 years ago.[16] Significantly, this is a ""minimum age constraint for human presence in Victoria"", and also could be interpreted as evidence for the oral histories of the Gunditjmara people, an Aboriginal Australian people of south-western Victoria, which tell of volcanic eruptions being some of the oldest oral traditions in existence.[17] An axe found underneath volcanic ash in 1947 had already proven that humans inhabited the region before the eruption of Tower Hill.[16] Oral literature is an ancient human tradition found in ""all corners of the world.""[18] Modern archaeology has been unveiling evidence of the human efforts to preserve and transmit arts and knowledge that depended completely or partially on an oral tradition, across various cultures: The Judeo-Christian Bible reveals its oral traditional roots; medieval European manuscripts are penned by performing scribes; geometric vases from archaic Greece mirror Homer's oral style. (...) Indeed, if these final decades of the millennium have taught us anything, it must be that oral tradition never was the other we accused it of being; it never was the primitive, preliminary technology of communication we thought it to be. Rather, if the whole truth is told, oral tradition stands out as the single most dominant communicative technology of our species as both a historical fact and, in many areas still, a contemporary reality.[18] The earliest poetry is believed to have been recited or sung, employed as a way of remembering history, genealogy, and law.[19] In Asia, the transmission of folklore, mythologies as well as scriptures in ancient India, in different Indian religions, was by oral tradition, preserved with precision with the help of elaborate mnemonic techniques.[20] The early Buddhist texts are also generally believed to be of oral tradition, with the first by comparing inconsistencies in the transmitted versions of literature from various oral societies such as the Greek, Serbia and other cultures, then noting that the Vedic literature is too consistent and vast to have been composed and transmitted orally across generations, without being written down.[21] According to Goody, the Vedic texts likely involved both a written and oral tradition, calling it a ""parallel products of a literate society"".[22] All ancient Greek literature was to some degree oral in nature, and the earliest literature was completely so.[23] Homer's epic poetry, states Michael Gagarin, was largely composed, performed and transmitted orally.[24] As folklores and legends were performed in front of distant audiences, the singers would substitute the names in the stories with local characters or rulers to give the stories a local flavor and thus connect with the audience by making the historicity embedded in the oral tradition as unreliable.[25] The lack of surviving texts about the Greek and Roman religious traditions have led scholars to presume that these were ritualistic and transmitted as oral traditions, but some scholars disagree that the complex rituals in the ancient Greek and Roman civilizations were an exclusive product of an oral tradition.[26] Writing systems are not known to have existed among Native North Americans (north of Mesoamerica) before contact with Europeans. Oral storytelling traditions flourished in a context without the use of writing to record and preserve history, scientific knowledge, and social practices.[27] While some"
Psychology,"Psychology is the scientific study of mind and behavior.[1][2] Its subject matter includes the behavior of humans and nonhumans, both conscious and unconscious phenomena, and mental processes such as thoughts, feelings, and motives. Psychology is an academic discipline of immense scope, crossing the boundaries between the natural and social sciences. Biological psychologists seek an understanding of the emergent properties of brains, linking the discipline to neuroscience. As social scientists, psychologists aim to understand the behavior of individuals and groups.[3][4] A professional practitioner or researcher involved in the discipline is called a psychologist. Some psychologists can also be classified as behavioral or cognitive scientists. Some psychologists attempt to understand the role of mental functions in individual and social behavior. Others explore the physiological and neurobiological processes that underlie cognitive functions and behaviors. Psychologists are involved in research on perception, cognition, attention, emotion, intelligence, subjective experiences, motivation, brain functioning, and personality. Psychologists' interests extend to interpersonal relationships, psychological resilience, family resilience, and other areas within social psychology. They also consider the unconscious mind.[5] Research psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. Some, but not all, clinical and counseling psychologists rely on symbolic interpretation. While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts, psychology ultimately aims to benefit society.[6][7][8] Many psychologists are involved in some kind of therapeutic role, practicing psychotherapy in clinical, counseling, or school settings. Other psychologists conduct scientific research on a wide range of topics related to mental processes and behavior. Typically the latter group of psychologists work in academic settings (e.g., universities, medical schools, or hospitals). Another group of psychologists is employed in industrial and organizational settings.[9] Yet others are involved in work on human development, aging, sports, health, forensic science, education, and the media. The word psychology derives from the Greek word psyche, for spirit or soul. The latter part of the word psychology derives from -λογία -logia, which means ""study"" or ""research"".[10] The word psychology was first used in the Renaissance.[11] In its Latin form psychiologia, it was first employed by the Croatian humanist and Latinist Marko Marulić in his book Psichiologia de ratione animae humanae (Psychology, on the Nature of the Human Soul) in the decade 1510–1520[11][12] The earliest known reference to the word psychology in English was by Steven Blankaart in 1694 in The Physical Dictionary. The dictionary refers to ""Anatomy, which treats the Body, and Psychology, which treats of the Soul.""[13] Ψ (psi), the first letter of the Greek word psyche from which the term psychology is derived, is commonly associated with the field of psychology. In 1890, William James defined psychology as ""the science of mental life, both of its phenomena and their conditions.""[14] This definition enjoyed widespread currency for decades. However, this meaning was contested, notably by John B. Watson, who in 1913 asserted the methodological behaviorist view of psychology as a purely objective experimental branch of natural science, the theoretical goal of which ""is the prediction and control of behavior.""[15] Since James defined ""psychology"", the term more strongly implicates scientific experimentation.[16][15] Folk psychology is the understanding of the mental states and behaviors of people held by ordinary people, as contrasted with psychology professionals' understanding.[17] The ancient civilizations of Egypt, Greece, China, India, and Persia all engaged in the philosophical study of psychology. In Ancient Egypt the Ebers Papyrus mentioned depression and thought disorders.[18] Historians note that Greek philosophers, including Thales, Plato, and Aristotle (especially in his De Anima treatise),[19] addressed the workings of the mind.[20] As early as the 4th century BC, the Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes.[21] In 387 BCE, Plato suggested that the brain is where mental processes take place, and in 335 BCE Aristotle suggested that it was the heart.[22] In China, psychological understanding grew from the philosophical works of Laozi and Confucius, and later from the doctrines of Buddhism.[23] This body of knowledge involves insights drawn from introspection and observation, as well as techniques for focused thinking and acting. It frames the universe in term of a division of physical reality and mental reality as well as the interaction between the physical and the mental.[citation needed] Chinese philosophy also emphasized purifying the mind in order to increase virtue and power. An ancient text known as The Yellow Emperor's Classic of Internal Medicine identifies the brain as the nexus of wisdom and sensation, includes theories of personality based on yin–yang balance, and analyzes mental disorder in terms of physiological and social disequilibria. Chinese scholarship that focused on the brain advanced during the Qing dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder with brain diseases, investigated the causes of dreams and insomnia, and advanced a theory of hemispheric lateralization in brain function.[24] Influenced by Hinduism, Indian philosophy explored distinctions in types of awareness. A central idea of the Upanishads and other Vedic texts that formed the foundations of Hinduism was the distinction between a person's transient mundane self and their eternal, unchanging soul. Divergent Hindu doctrines and Buddhism have challenged this hierarchy of selves, but have all emphasized the importance of reaching higher awareness. Yoga encompasses a range of techniques used in pursuit of this goal. Theosophy, a religion established by Russian-American philosopher Helena Blavatsky, drew inspiration from these doctrines during her time in British India.[25][26] Psychology was of interest to Enlightenment thinkers in Europe. In Germany, Gottfried Wilhelm Leibniz (1646–1716) applied his principles of calculus to the mind, arguing that mental activity took place on an indivisible continuum. He suggested that the difference between conscious and unconscious awareness is only a matter of degree. Christian Wolff identified psychology as its own science, writing Psychologia Empirica in 1732 and Psychologia Rationalis in 1734. Immanuel Kant"
Politics,"This is an accepted version of this page Politics (from Ancient Greek πολιτικά (politiká) 'affairs of the cities') is the set of activities that are associated with making decisions in groups, or other forms of power relations among individuals, such as the distribution of status or resources. The branch of social science that studies politics and government is referred to as political science. Politics may be used positively in the context of a ""political solution"" which is compromising and non-violent,[1] or descriptively as ""the art or science of government"", but the word often also carries a negative connotation.[2] The concept has been defined in various ways, and different approaches have fundamentally differing views on whether it should be used extensively or in a limited way, empirically or normatively, and on whether conflict or co-operation is more essential to it. A variety of methods are deployed in politics, which include promoting one's own political views among people, negotiation with other political subjects, making laws, and exercising internal and external force, including warfare against adversaries.[3][4][5][6][7] Politics is exercised on a wide range of social levels, from clans and tribes of traditional societies, through modern local governments, companies and institutions up to sovereign states, to the international level. In modern nation states, people often form political parties to represent their ideas. Members of a party often agree to take the same position on many issues and agree to support the same changes to law and the same leaders. An election is usually a competition between different parties. A political system is a framework which defines acceptable political methods within a society. The history of political thought can be traced back to early antiquity, with seminal works such as Plato's Republic, Aristotle's Politics, Confucius's political manuscripts and Chanakya's Arthashastra.[8] The English word politics has its roots in the name of Aristotle's classic work, Politiká, which introduced the Ancient Greek term politiká (Πολιτικά, 'affairs of the cities'). In the mid-15th century, Aristotle's composition was rendered in Early Modern English as Polettiques [sic],[a][9] which became Politics in Modern English. The singular politic first attested in English in 1430, coming from Middle French politique—itself taking from politicus,[10] a Latinization of the Greek πολιτικός (politikos) from πολίτης (polites, 'citizen') and πόλις (polis, 'city').[11] There are several ways in which approaching politics has been conceptualized. Adrian Leftwich has differentiated views of politics based on how extensive or limited their perception of what accounts as 'political' is.[18] The extensive view sees politics as present across the sphere of human social relations, while the limited view restricts it to certain contexts. For example, in a more restrictive way, politics may be viewed as primarily about governance,[19] while a feminist perspective could argue that sites which have been viewed traditionally as non-political, should indeed be viewed as political as well.[20] This latter position is encapsulated in the slogan ""the personal is political"", which disputes the distinction between private and public issues. Politics may also be defined by the use of power, as has been argued by Robert A. Dahl.[21] Some perspectives on politics view it empirically as an exercise of power, while others see it as a social function with a normative basis.[22] This distinction has been called the difference between political moralism and political realism.[23] For moralists, politics is closely linked to ethics, and is at its extreme in utopian thinking.[23] For example, according to Hannah Arendt, the view of Aristotle was that, ""to be political…meant that everything was decided through words and persuasion and not through violence"";[24] while according to Bernard Crick, ""politics is the way in which free societies are governed. Politics is politics, and other forms of rule are something else.""[25] In contrast, for realists, represented by those such as Niccolò Machiavelli, Thomas Hobbes, and Harold Lasswell, politics is based on the use of power, irrespective of the ends being pursued.[26][23] Agonism argues that politics essentially comes down to conflict between conflicting interests. Political scientist Elmer Schattschneider argued that ""at the root of all politics is the universal language of conflict"",[27] while for Carl Schmitt the essence of politics is the distinction of 'friend' from 'foe'.[28] This is in direct contrast to the more co-operative views of politics by Aristotle and Crick. However, a more mixed view between these extremes is provided by Irish political scientist Michael Laver, who noted that: Politics is about the characteristic blend of conflict and co-operation that can be found so often in human interactions. Pure conflict is war. Pure co-operation is true love. Politics is a mixture of both.[29] The history of politics spans human history and is not limited to modern institutions of government. Frans de Waal argued that chimpanzees engage in politics through ""social manipulation to secure and maintain influential positions"".[31] Early human forms of social organization—bands and tribes—lacked centralized political structures.[32] These are sometimes referred to as stateless societies. In ancient history, civilizations did not have definite boundaries as states have today, and their borders could be more accurately described as frontiers. Early dynastic Sumer, and early dynastic Egypt were the first civilizations to define their borders. Moreover, up to the 12th century, many people lived in non-state societies. These range from relatively egalitarian bands and tribes to complex and highly stratified chiefdoms. There are a number of different theories and hypotheses regarding early state formation that seek generalizations to explain why the state developed in some places but not others. Other scholars believe that generalizations are unhelpful and that each case of early state formation should be treated on its own.[33] Voluntary theories contend that diverse groups of people came together to form states as a result of some shared rational interest.[34] The theories largely focus on the development of agriculture, and the population and organizational pressure that followed and resulted in state formation. One of the most prominent theories of early and primary state formation is the hydraulic hypothesis, which contends that the state was a result of the need to build and maintain large-scale irrigation projects.[35]"
Economics,"Empirical methods Prescriptive and policy Economics (/ˌɛkəˈnɒmɪks, ˌiːkə-/)[1][2] is a social science that studies the production, distribution, and consumption of goods and services.[3][4] Economics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyses what is viewed as basic elements within economies, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyses economies as systems where production, distribution, consumption, savings, and investment expenditure interact, and factors affecting it: factors of production, such as labour, capital, land, and enterprise, inflation, economic growth, and public policies that have impact on these elements. It also seeks to analyse and describe the global economy. Other broad distinctions within economics include those between positive economics, describing ""what is"", and normative economics, advocating ""what ought to be"";[5] between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics.[6] Economic analysis can be applied throughout society, including business,[7] finance, cybersecurity,[8] health care,[9] engineering[10] and government.[11] It is also applied to such diverse subjects as crime,[12] education,[13] the family,[14] feminism,[15] law,[16] philosophy,[17] politics, religion,[18] social institutions, war,[19] science,[20] and the environment.[21] The earlier term for the discipline was ""political economy"", but since the late 19th century, it has commonly been called ""economics"".[22] The term is ultimately derived from Ancient Greek οἰκονομία (oikonomia) which is a term for the ""way (nomos) to run a household (oikos)"", or in other words the know-how of an οἰκονομικός (oikonomikos), or ""household or homestead manager"". Derived terms such as ""economy"" can therefore often mean ""frugal"" or ""thrifty"".[23][24][25][26] By extension then, ""political economy"" was the way to manage a polis or state. There are a variety of modern definitions of economics; some reflect evolving views of the subject or different views among economists.[27][28] Scottish philosopher Adam Smith (1776) defined what was then called political economy as ""an inquiry into the nature and causes of the wealth of nations"", in particular as: a branch of the science of a statesman or legislator [with the twofold objectives of providing] a plentiful revenue or subsistence for the people ... [and] to supply the state or commonwealth with a revenue for the publick services.[29] Jean-Baptiste Say (1803), distinguishing the subject matter from its public-policy uses, defined it as the science of production, distribution, and consumption of wealth.[30] On the satirical side, Thomas Carlyle (1849) coined ""the dismal science"" as an epithet for classical economics, in this context, commonly linked to the pessimistic analysis of Malthus (1798).[31] John Stuart Mill (1844) delimited the subject matter further: The science which traces the laws of such of the phenomena of society as arise from the combined operations of mankind for the production of wealth, in so far as those phenomena are not modified by the pursuit of any other object.[32] Alfred Marshall provided a still widely cited definition in his textbook Principles of Economics (1890) that extended analysis beyond wealth and from the societal to the microeconomic level: Economics is a study of man in the ordinary business of life. It enquires how he gets his income and how he uses it. Thus, it is on the one side, the study of wealth and on the other and more important side, a part of the study of man.[33] Lionel Robbins (1932) developed implications of what has been termed ""[p]erhaps the most commonly accepted current definition of the subject"":[28] Economics is the science which studies human behaviour as a relationship between ends and scarce means which have alternative uses.[34] Robbins described the definition as not classificatory in ""pick[ing] out certain kinds of behaviour"" but rather analytical in ""focus[ing] attention on a particular aspect of behaviour, the form imposed by the influence of scarcity.""[35] He affirmed that previous economists have usually centred their studies on the analysis of wealth: how wealth is created (production), distributed, and consumed; and how wealth can grow.[36] But he said that economics can be used to study other things, such as war, that are outside its usual focus. This is because war has as the goal winning it (as a sought after end), generates both cost and benefits; and, resources (human life and other costs) are used to attain the goal. If the war is not winnable or if the expected costs outweigh the benefits, the deciding actors (assuming they are rational) may never go to war (a decision) but rather explore other alternatives. Economics cannot be defined as the science that studies wealth, war, crime, education, and any other field economic analysis can be applied to; but, as the science that studies a particular common aspect of each of those subjects (they all use scarce resources to attain a sought after end). Some subsequent comments criticised the definition as overly broad in failing to limit its subject matter to analysis of markets. From the 1960s, however, such comments abated as the economic theory of maximizing behaviour and rational-choice modelling expanded the domain of the subject to areas previously treated in other fields.[37] There are other criticisms as well, such as in scarcity not accounting for the macroeconomics of high unemployment.[38] Gary Becker, a contributor to the expansion of economics into new areas, described the approach he favoured as ""combin[ing the] assumptions of maximizing behaviour, stable preferences, and market equilibrium, used relentlessly and unflinchingly.""[39] One commentary characterises the remark as making economics an approach rather than a subject matter but with great specificity as to the ""choice process and the type of social interaction that [such] analysis involves."" The same source reviews a range of definitions included in principles of economics textbooks and concludes that the lack of agreement need not affect the subject-matter that the texts treat. Among economists more generally, it argues that a particular definition presented may reflect the direction toward which the author believes economics is evolving, or should evolve.[28] Many economists including nobel prize winners James M. Buchanan and Ronald Coase reject the method-based definition of"
Sociology,"1800s: Martineau · Tocqueville · Marx · Spencer · Le Bon · Ward · Pareto · Tönnies · Veblen · Simmel · Durkheim · Addams · Mead · Weber · Du Bois · Mannheim · Elias Sociology is the scientific study of human society that focuses on society, human social behavior, patterns of social relationships, social interaction, and aspects of culture associated with everyday life.[1][2][3] Regarded as a part of both the social sciences and humanities, sociology uses various methods of empirical investigation and critical analysis[4]: 3–5 to develop a body of knowledge about social order and social change.[4]: 32–40 Sociological subject matter ranges from micro-level analyses of individual interaction and agency to macro-level analyses of social systems and social structure. Applied sociological research may be applied directly to social policy and welfare, whereas theoretical approaches may focus on the understanding of social processes and phenomenological method.[5] Traditional focuses of sociology include social stratification, social class, social mobility, religion, secularization, law, sexuality, gender, and deviance. Recent studies have added socio-technical aspects of the digital divide as a new focus.[6] As all spheres of human activity are affected by the interplay between social structure and individual agency, sociology has gradually expanded its focus to other subjects and institutions, such as health and the institution of medicine; economy; military; punishment and systems of control; the Internet; sociology of education; social capital; and the role of social activity in the development of scientific knowledge. The range of social scientific methods has also expanded, as social researchers draw upon a variety of qualitative and quantitative techniques. The linguistic and cultural turns of the mid-20th century, especially, have led to increasingly interpretative, hermeneutic, and philosophical approaches towards the analysis of society. Conversely, the turn of the 21st century has seen the rise of new analytically, mathematically, and computationally rigorous techniques, such as agent-based modelling and social network analysis.[7][8] Social research has influence throughout various industries and sectors of life, such as among politicians, policy makers, and legislators; educators; planners; administrators; developers; business magnates and managers; social workers; non-governmental organizations; and non-profit organizations, as well as individuals interested in resolving social issues in general. Sociological reasoning predates the foundation of the discipline itself. Social analysis has origins in the common stock of universal, global knowledge and philosophy, having been carried out from as far back as the time of old comic poetry which features social and political criticism,[9] and ancient Greek philosophers Socrates, Plato, and Aristotle. For instance, the origin of the survey can be traced back to at least the Domesday Book in 1086,[10][11] while ancient philosophers such as Confucius wrote about the importance of social roles.[12][13] Medieval Arabic writings encompass a rich tradition that unveils early insights into the field of sociology. Some sources consider Ibn Khaldun, a 14th-century Muslim scholar from Tunisia,[note 1] to have been the father of sociology, although there is no reference to his work in the writings of European contributors to modern sociology.[14][15][16][17] Khaldun's Muqaddimah was considered to be amongst the first works to advance social-scientific reasoning on social cohesion and social conflict.[18][19][20][21][22][23] The word sociology derives part of its name from the Latin word socius ('companion' or 'fellowship'[24]). The suffix -logy ('the study of') comes from that of the Greek -λογία, derived from λόγος (lógos, 'word' or 'knowledge').[citation needed] The term sociology was first coined in 1780 by the French essayist Emmanuel-Joseph Sieyès in an unpublished manuscript.[25][note 2] Sociology was later defined independently by French philosopher of science Auguste Comte (1798–1857) in 1838[26] as a new way of looking at society.[27]: 10 Comte had earlier used the term social physics, but it had been subsequently appropriated by others, most notably the Belgian statistician Adolphe Quetelet.[28] Comte endeavored to unify history, psychology, and economics through the scientific understanding of social life. Writing shortly after the malaise of the French Revolution, he proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in the Course in Positive Philosophy (1830–1842), later included in A General View of Positivism (1848). Comte believed a positivist stage would mark the final era in the progression of human understanding, after conjectural theological and metaphysical phases.[29] In observing the circular dependence of theory and observation in science, and having classified the sciences, Comte may be regarded as the first philosopher of science in the modern sense of the term.[30][31] Comte gave a powerful impetus to the development of sociology, an impetus that bore fruit in the later decades of the nineteenth century. To say this is certainly not to claim that French sociologists such as Durkheim were devoted disciples of the high priest of positivism. But by insisting on the irreducibility of each of his basic sciences to the particular science of sciences which it presupposed in the hierarchy and by emphasizing the nature of sociology as the scientific study of social phenomena Comte put sociology on the map. To be sure, [its] beginnings can be traced back well beyond Montesquieu, for example, and to Condorcet, not to speak of Saint-Simon, Comte's immediate predecessor. But Comte's clear recognition of sociology as a particular science, with a character of its own, justified Durkheim in regarding him as the father or founder of this science, even though Durkheim did not accept the idea of the three states and criticized Comte's approach to sociology. Both Comte and Karl Marx set out to develop scientifically justified systems in the wake of European industrialization and secularization, informed by various key movements in the philosophies of history and science. Marx rejected Comtean positivism[32] but in attempting to develop a ""science of society"" nevertheless came to be recognized as a founder of sociology as the word gained wider meaning. For Isaiah Berlin, even though Marx did not consider himself to be a sociologist, he may be regarded as the ""true father"" of modern sociology, ""in so far as anyone can claim the title.""[33]: 130 To have given clear and unified answers in familiar empirical terms to those theoretical questions"

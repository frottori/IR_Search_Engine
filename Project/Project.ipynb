{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 1: Συλλογή Δεδομένων"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H συνάρτηση Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_wikipedia(url):\n",
    "    data = []\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Εξαγωγή τίτλου και κειμένου\n",
    "        title = soup.find(\"h1\").text # βρίσκει h1 html tag (header1) και επιστρέφει το κείμενο του\n",
    "        paragraphs = [p.text for p in soup.find_all(\"p\")] # βρίσκει όλα τα p html tags (paragraph) και επιστρέφει το κείμενο τους\n",
    "        content = \"\\n\".join(paragraphs) # διαχωριστής των παραγράφων το σύμβολο \" | \"\n",
    "        data.append({'title': title, 'content': content}) # προσθήκη τίτλου και περιεχομένου στη λίστα data\n",
    "    else: \n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(f\"URL: {url}\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Χρηση της συνάρτησης Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Λίστα άρθρων για συλλογή \n",
    "articles = [\n",
    "    \"Science\",\n",
    "    \"Technology\",\n",
    "    \"Engineering\",\n",
    "    \"Mathematics\",\n",
    "    \"Artificial_intelligence\",\n",
    "    \"Machine_learning\",\n",
    "    \"Deep_learning\",\n",
    "    \"Data_science\",\n",
    "    \"Computer_science\",\n",
    "    \"Programming_language\",\n",
    "    \"Software_engineering\",\n",
    "    \"Operating_system\",\n",
    "    \"Computer_network\",\n",
    "    \"Internet\",\n",
    "]\n",
    "collected_data = []\n",
    "\n",
    "for article in articles:\n",
    "    url = f'https://en.wikipedia.org/wiki/{article}'\n",
    "    collected_data.extend(crawl_wikipedia(url))\n",
    "    # for d in collected_data:\n",
    "    #     words = d['content'].split()\n",
    "    #     d['content'] = \" \".join(words[:500]) # each article is limited to 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles collected: 14\n",
      "1.Article Title: Science\n",
      "  Content (first 100 words):\n",
      "('Science is a systematic discipline that builds and organises knowledge in '\n",
      " 'the form of testable hypotheses and predictions about the universe.[1][2] '\n",
      " 'Modern science is typically divided into two or three major branches:[3] the '\n",
      " 'natural sciences (e.g., physics, chemistry, and biology), which study the '\n",
      " 'physical world; and the behavioural sciences (e.g., economics, psychology, '\n",
      " 'and sociology), which study individuals and societies.[4][5] The formal '\n",
      " 'sciences (e.g., logic, mathematics, and theoretical computer science), which '\n",
      " 'study formal systems governed by axioms and rules,[6][7] are sometimes '\n",
      " 'described as being sciences as well; however, they are often regarded as a '\n",
      " 'separate field because they rely on deductive')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "def print_article(collected_data):\n",
    "    print(\"Number of articles collected:\", len(collected_data))\n",
    "    print(f\"1.Article Title: {collected_data[0]['title']}\")\n",
    "    print(\"  Content (first 100 words):\")\n",
    "    pprint(\" \".join(collected_data[0]['content'].split()[:100]))\n",
    "print_article(collected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αποθήκευση σε JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_json(data, filename):\n",
    "    with open('Files/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "# Αποθήκευση δεδομένων σε json αρχείο\n",
    "save_json(collected_data, 'wiki_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αποθήκευση σε CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def save_csv(data, filename):\n",
    "    with open('Files/' + filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['title', 'content'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Αποθήκευση δεδομένων σε csv αρχείο\n",
    "save_csv(collected_data, 'wiki_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 2: Προεπεξεργασία Κειμένου (Text Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αφαιρεση πηγών (π.χ. [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles collected: 14\n",
      "1.Article Title: Science\n",
      "  Content (first 100 words):\n",
      "('Science is a systematic discipline that builds and organises knowledge in '\n",
      " 'the form of testable hypotheses and predictions about the universe. Modern '\n",
      " 'science is typically divided into two or three major branches: the natural '\n",
      " 'sciences (e.g., physics, chemistry, and biology), which study the physical '\n",
      " 'world; and the behavioural sciences (e.g., economics, psychology, and '\n",
      " 'sociology), which study individuals and societies. The formal sciences '\n",
      " '(e.g., logic, mathematics, and theoretical computer science), which study '\n",
      " 'formal systems governed by axioms and rules, are sometimes described as '\n",
      " 'being sciences as well; however, they are often regarded as a separate field '\n",
      " 'because they rely on deductive')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import re \n",
    "import pandas as pd\n",
    "\n",
    "# Load collected data from CSV\n",
    "collected_data_df = pd.read_csv('Files/wiki_data.csv')\n",
    "collected_data = collected_data_df.to_dict(orient='records')\n",
    "for d in collected_data:\n",
    "    d['content'] = re.sub(r\"\\[\\d+\\]\", \"\", d['content']) # regex για αντικατάσταση πηγών με κενό\n",
    "\n",
    "print_article(collected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αφαίρεση σημείων στίξης"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles collected: 14\n",
      "1.Article Title: Science\n",
      "  Content (first 100 words):\n",
      "('Science is a systematic discipline that builds and organises knowledge in '\n",
      " 'the form of testable hypotheses and predictions about the universe Modern '\n",
      " 'science is typically divided into two or three major branches the natural '\n",
      " 'sciences eg physics chemistry and biology which study the physical world and '\n",
      " 'the behavioural sciences eg economics psychology and sociology which study '\n",
      " 'individuals and societies The formal sciences eg logic mathematics and '\n",
      " 'theoretical computer science which study formal systems governed by axioms '\n",
      " 'and rules are sometimes described as being sciences as well however they are '\n",
      " 'often regarded as a separate field because they rely on deductive')\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "for punct in string.punctuation:\n",
    "    for d in collected_data:\n",
    "        d['content'] = d['content'].replace(punct, '') # αφαίρεση σημείων στίξης\n",
    "print_article(collected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "tokens = []\n",
    "stemmed_data = []\n",
    "\n",
    "for d in collected_data: # For each article\n",
    "    tokens = word_tokenize(d['content'])  # Tokenize content \n",
    "    stemmed_tokens = [porter.stem(t) for t in tokens]  # Stem each token\n",
    "    stemmed_data.append({\n",
    "        \"title\": d[\"title\"],\n",
    "        \"stemmed_tokens\": stemmed_tokens\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Article Title: Science\n",
      "Tokens first 20 words: \n",
      "['scienc',\n",
      " 'is',\n",
      " 'a',\n",
      " 'systemat',\n",
      " 'disciplin',\n",
      " 'that',\n",
      " 'build',\n",
      " 'and',\n",
      " 'organis',\n",
      " 'knowledg',\n",
      " 'in',\n",
      " 'the',\n",
      " 'form',\n",
      " 'of',\n",
      " 'testabl',\n",
      " 'hypothes',\n",
      " 'and',\n",
      " 'predict',\n",
      " 'about',\n",
      " 'the']\n"
     ]
    }
   ],
   "source": [
    "def print_tokens(data, tokens):\n",
    "    print(f\"1. Article Title: {data[0]['title']}\")\n",
    "    print(\"Tokens first 20 words: \")\n",
    "    pprint(data[0][tokens][:20])\n",
    "print_tokens(stemmed_data, \"stemmed_tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Article Title: Science\n",
      "Tokens first 20 words: \n",
      "['scienc',\n",
      " 'systemat',\n",
      " 'disciplin',\n",
      " 'build',\n",
      " 'organis',\n",
      " 'knowledg',\n",
      " 'form',\n",
      " 'testabl',\n",
      " 'hypothes',\n",
      " 'predict',\n",
      " 'univers',\n",
      " 'modern',\n",
      " 'scienc',\n",
      " 'typic',\n",
      " 'divid',\n",
      " 'two',\n",
      " 'three',\n",
      " 'major',\n",
      " 'branch',\n",
      " 'natur']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "cleaned_data = []\n",
    "cleaned_data_for_csv = []\n",
    "\n",
    "for d in stemmed_data: # or lemmed_data (one of the two) - Επιλογή μεταξύ stemming και lemmatization??\n",
    "    filtered_tokens = [t for t in d['stemmed_tokens'] if t.lower() not in stopwords]\n",
    "    cleaned_data.append({\n",
    "        \"title\": d[\"title\"],\n",
    "        \"cleaned_tokens\": filtered_tokens\n",
    "    })\n",
    "    cleaned_data_for_csv.append({\n",
    "        \"title\": d[\"title\"],\n",
    "        \"content\": \" \".join(filtered_tokens)\n",
    "    })\n",
    "    \n",
    "print_tokens(cleaned_data, \"cleaned_tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αποθήκευση σε .json και .csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(cleaned_data, 'wiki_data_cleaned.json')\n",
    "save_csv(cleaned_data_for_csv, 'wiki_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 3: Ευρετήριο (Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 columns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artifici</th>\n",
       "      <th>intellig</th>\n",
       "      <th>ai</th>\n",
       "      <th>broadest</th>\n",
       "      <th>sens</th>\n",
       "      <th>exhibit</th>\n",
       "      <th>machin</th>\n",
       "      <th>particularli</th>\n",
       "      <th>comput</th>\n",
       "      <th>system</th>\n",
       "      <th>field</th>\n",
       "      <th>research</th>\n",
       "      <th>scienc</th>\n",
       "      <th>develop</th>\n",
       "      <th>studi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Artificial intelligence</th>\n",
       "      <td>41</td>\n",
       "      <td>73</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>34</td>\n",
       "      <td>22</td>\n",
       "      <td>53</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computer network</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computer science</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data science</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep learning</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engineering</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Internet</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machine learning</th>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathematics</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Operating system</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Programming language</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>53</td>\n",
       "      <td>131</td>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Software engineering</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Technology</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         artifici  intellig   ai  broadest  sens  exhibit  \\\n",
       "Artificial intelligence        41        73  221         1     4        3   \n",
       "Computer network                0         0    0         0     0        1   \n",
       "Computer science                6         8    2         0     0        0   \n",
       "Data science                    0         0    0         0     0        0   \n",
       "Deep learning                  13         4    8         0     1        0   \n",
       "Engineering                     3         1    0         0     1        2   \n",
       "Internet                        0         1    0         0     0        1   \n",
       "Machine learning               28        13   23         0     0        1   \n",
       "Mathematics                     0         0    0         0     1        0   \n",
       "Operating system                0         0    0         0     0        0   \n",
       "Programming language            1         1    0         0     0        1   \n",
       "Science                         2         1    0         0     4        1   \n",
       "Software engineering            1         1    0         0     0        0   \n",
       "Technology                      6         7   10         0     1        0   \n",
       "\n",
       "                         machin  particularli  comput  system  field  \\\n",
       "Artificial intelligence      59             5      41      34     22   \n",
       "Computer network              2             0      48      17      1   \n",
       "Computer science             13             0     166      28     15   \n",
       "Data science                  6             0      16       4     18   \n",
       "Deep learning                25             4      35      38      8   \n",
       "Engineering                  28             0      13      21     15   \n",
       "Internet                      0             0      43      26      5   \n",
       "Machine learning            119             3      37      40     24   \n",
       "Mathematics                   0             4      23      13     11   \n",
       "Operating system              8             0      48     166      0   \n",
       "Programming language         13             1      30      11      3   \n",
       "Science                       3             0       9      11     13   \n",
       "Software engineering          0             0      33       7     10   \n",
       "Technology                    8             2      12       8      6   \n",
       "\n",
       "                         research  scienc  develop  studi  \n",
       "Artificial intelligence        53       6       36     11  \n",
       "Computer network                2       1        3      1  \n",
       "Computer science               10      70       13     22  \n",
       "Data science                    3      56        4      0  \n",
       "Deep learning                  19       4       19      1  \n",
       "Engineering                     6      25       40     12  \n",
       "Internet                       23       5       19     11  \n",
       "Machine learning               20       3        7     11  \n",
       "Mathematics                     4      28       23     47  \n",
       "Operating system                1       0       14      0  \n",
       "Programming language            2       2        9      1  \n",
       "Science                        53     131       28     24  \n",
       "Software engineering            4      13       26      1  \n",
       "Technology                     17       9       19      8  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('Files/wiki_data_cleaned.json', 'r', encoding='utf-8') as file:\n",
    "    wiki_data = json.load(file)\n",
    "\n",
    "corpus = {}\n",
    "for i, entry in enumerate(wiki_data):\n",
    "    title = entry.get(\"title\", f\"sent{i}\") \n",
    "    tokens = entry.get(\"cleaned_tokens\", [])\n",
    "    corpus[title] = {token: tokens.count(token) for token in tokens}\n",
    "\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "print(\"First 15 columns:\")\n",
    "df.iloc[:, :15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αποθήκευση σε .json και .csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Files/wiki_data_inverted_index.csv')\n",
    "df.to_json('Files/wiki_data_inverted_index.json', indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 4: Μηχανή αναζήτησης (Search Engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Επεξεργασία ερωτήματος (Query Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Λειτουργίες μηχανής αναζήτησης\n",
    "def boolean_query(query, index):\n",
    "    \"\"\"Boolean (AND, OR, NOT)\"\"\"\n",
    "    terms = query.split()\n",
    "    result_sets = []\n",
    "\n",
    "    for term in terms:\n",
    "        if term in index:\n",
    "            stemmer = PorterStemmer()\n",
    "            term = stemmer.stem(term)\n",
    "            result_sets.append(set(index[term]))\n",
    "        elif term.upper() == \"AND\":\n",
    "            continue\n",
    "        elif term.upper() == \"OR\":\n",
    "            continue\n",
    "        else:\n",
    "            result_sets.append(set())\n",
    "\n",
    "    result = set.intersection(*result_sets) if \"AND\" in terms else set.union(*result_sets)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Κατάταξη αποτελεσμάτων (Ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Preprocess the input query (tokenize and stem)\n",
    "def preprocess_query(query):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(query.lower())  # Tokenize and lowercased\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Function to calculate TF-IDF\n",
    "def calculate_tfidf(query, inverted_index):\n",
    "    # Preprocess query\n",
    "    query_tokens = preprocess_query(query)\n",
    "\n",
    "    # Build the corpus from the inverted index\n",
    "    corpus = []\n",
    "    document_names = list(next(iter(inverted_index.values())).keys())  # Extract document names\n",
    "\n",
    "    # Create the document-term matrix\n",
    "    for doc in document_names:\n",
    "        doc_str = \" \".join(\n",
    "            [term for term, docs in inverted_index.items() if doc in docs and docs[doc] > 0]\n",
    "        )\n",
    "        corpus.append(doc_str)\n",
    "\n",
    "    # Apply TF-IDF using TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Query TF-IDF transformation\n",
    "    query_vector = vectorizer.transform([\" \".join(query_tokens)])\n",
    "\n",
    "    # Calculate cosine similarity between query and documents\n",
    "    cosine_similarities = cosine_similarity(query_vector, X).flatten()\n",
    "\n",
    "    # Rank documents based on cosine similarity\n",
    "    ranked_docs = sorted(zip(cosine_similarities, document_names), reverse=True)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vector Space Model (VSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_space_model(query, inverted_index):\n",
    "    # Step 1: Preprocess the query\n",
    "    preprocessed_query = preprocess_query(query)\n",
    "    processed_query = \" \".join(preprocessed_query)\n",
    "\n",
    "    # Step 2: Prepare the document-term matrix\n",
    "    documents = list(inverted_index.values())\n",
    "    doc_names = list(documents[0].keys())  # Extract document names from the first term\n",
    "\n",
    "    # Create a list of document strings (terms concatenated)\n",
    "    doc_texts = [\n",
    "        \" \".join(\n",
    "            [term] * inverted_index[term].get(doc, 0)\n",
    "            for term in inverted_index\n",
    "        )\n",
    "        for doc in doc_names\n",
    "    ]\n",
    "\n",
    "    # Step 3: Compute TF-IDF representation\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(doc_texts)\n",
    "\n",
    "    # Step 4: Transform the query into the same vector space\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "\n",
    "    # Step 5: Compute cosine similarity between the query and the documents\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Step 6: Rank the documents by similarity scores\n",
    "    ranked_docs = sorted(zip(doc_names, cosine_similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okapi BM25 (Probabilistic retrieval model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 5. Αξιολόγηση συστήματος:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αξιολόγηση συστήματος με Precision, Recall, F1, MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Διεπαφή Χρήστη (User Interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Διάβασμα άρθρων και ευρετηρίου"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "user_queries = []\n",
    "user_relevant_docs = []\n",
    "\n",
    "# Load articles\n",
    "with open(\"Files/wiki_data_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "# Load inverted index\n",
    "with open(\"Files/wiki_data_inverted_index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    inverted_index = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Διεπαφή χρήστη"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_interface():\n",
    "    print(\"\\n\")\n",
    "    while True:\n",
    "        questions = [\n",
    "            \"----------Search Engine Menu----------\",\n",
    "            \"1. Boolean search\",\n",
    "            \"2. TF-IDF Ranking\",\n",
    "            \"4. Vector Space Model Ranking\",\n",
    "            \"5. Okapi BM25\"\n",
    "            \"6. System Evaluation\",\n",
    "            \"7. Exit\",\n",
    "        ]\n",
    "        print(\"\\n\".join(questions))\n",
    "        choice = input(\"Choose:\")\n",
    "\n",
    "        if choice == \"1\":\n",
    "            query = input(\"Input Boolean query (e.g. term1 AND term2): \")\n",
    "            results = boolean_query(query, inverted_index)\n",
    "            if not results:\n",
    "                print(\"No results found.\")\n",
    "                break\n",
    "\n",
    "            print(\"Αποτελέσματα:\")\n",
    "            for res in results:\n",
    "                print(res)\n",
    "            user_queries.append(query)\n",
    "            user_relevant_docs.append(list(results))\n",
    "        elif choice == \"2\":\n",
    "            query = input(\"Input query:\")\n",
    "            print(\"----------Results TF-IDF----------\")\n",
    "            print(\"Query given:\", query)\n",
    "            ranked_docs = calculate_tfidf(query, inverted_index)\n",
    "            if not ranked_docs:\n",
    "                print(\"No results found.\")\n",
    "                break\n",
    "\n",
    "            for score, doc in ranked_docs:\n",
    "                    print(f\"Document: {doc}, Score: {score:.4f}\")\n",
    "            user_queries.append(query)\n",
    "            user_relevant_docs.append([doc for _, doc in ranked_docs])  \n",
    "        elif choice == \"3\":\n",
    "            query = input(\"Input query:\")\n",
    "            print(\"----------Results Vector Space Model----------\")\n",
    "            print(\"Query given:\", query)\n",
    "            ranked_docs = vector_space_model(query, inverted_index)\n",
    "            if not ranked_docs:\n",
    "                print(\"No results found.\")\n",
    "                break\n",
    "\n",
    "            for doc, score in ranked_docs:\n",
    "                print(f\"Document: {doc}, Score: {score:.4f}\")\n",
    "            user_queries.append(query)\n",
    "            user_relevant_docs.append([doc for doc, _ in ranked_docs])      \n",
    "        elif choice == \"7\" or choice == \"\": \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------Search Engine Menu----------\n",
      "1. Boolean search\n",
      "2. TF-IDF Ranking\n",
      "4. Vector Space Model Ranking\n",
      "5. Okapi BM256. System Evaluation\n",
      "7. Exit\n",
      "----------Results Vector Space Model----------\n",
      "Query given: artificial intelligence\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43muser_interface\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 45\u001b[0m, in \u001b[0;36muser_interface\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------Results Vector Space Model----------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery given:\u001b[39m\u001b[38;5;124m\"\u001b[39m, query)\n\u001b[1;32m---> 45\u001b[0m ranked_docs \u001b[38;5;241m=\u001b[39m \u001b[43mvector_space_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverted_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ranked_docs:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo results found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[33], line 12\u001b[0m, in \u001b[0;36mvector_space_model\u001b[1;34m(query, inverted_index)\u001b[0m\n\u001b[0;32m      8\u001b[0m doc_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(documents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())  \u001b[38;5;66;03m# Extract document names from the first term\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create a list of document strings (terms concatenated)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m doc_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minverted_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minverted_index\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m doc_names\n\u001b[0;32m     17\u001b[0m ]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Step 3: Compute TF-IDF representation\u001b[39;00m\n\u001b[0;32m     20\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "user_interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
